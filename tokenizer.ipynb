{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f524f8b-2490-403f-970a-27f58df09cfb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Subwords Tokenizing\n",
    "- google sentencepiece\n",
    "- huggingface sentencepiece\n",
    "- normally 25,000 ~ 35,000 words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbcc9d5-00d8-4b32-9e58-85bc135dca3a",
   "metadata": {},
   "source": [
    "### 종류\n",
    "\n",
    "#### 1. BPE (Byte-Pair Encoding)\n",
    "- 공백 기준으로 pre-tokenize를 거친 후 가장 함께 많이 등장한 character pair를 사전에 추가\n",
    "\n",
    "#### 2. Wordpiece\n",
    "- bert에서 사용\n",
    "- 사용자가 지정한 횟수 만큼 subwords를 졍합하는 방식. bpe와 다른 점은 bpe는 같이 많이 등장한 쌍을 병합하지만 wordpiece는 병합되었을 때 corpus의 likelihood를 가장 높이는 쌍을 병합\n",
    "\n",
    "#### 3. Sentencepiece\n",
    "- wordpiece, sentencepiece는 BPE의 작은 변형들\n",
    "    ##### Unigram\n",
    "    - bpe, wordpiece와 같이 기본 character에서 subwords를 점진적으로 병합하는 것이 아니라, 모든 pre-tokenized token과 subwords에서 시작해 점차 사전을 줄여나가는 방식으로 진행\n",
    "    - SentencePiece에서 활용되는 알고리즘\n",
    "    - 대부분 한국어는 wordpiece보다 sentencepiece를 이용한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d715ea-8abc-42ab-9264-4988dff634f3",
   "metadata": {},
   "source": [
    "### make corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb4ba613-d785-4024-8872-e23a7f7d90c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from dbConn.mongo_conn import config\n",
    "\n",
    "conn = config()\n",
    "col = conn[\"travel_ai\"].blog_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696243ca-80aa-4044-9b13-d8919854532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "162013it [00:03, 44571.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "108118909"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents = col.find({\"num_docs\": {\"$gt\": 1}}, {\"cleaned_content\": 1})\n",
    "input_f = \"./data/for_tokenizer_corpus.txt\"\n",
    "f = open(input_f, \"w\")\n",
    "for cont in contents:\n",
    "    docs = [c for c in cont['cleaned_content']]\n",
    "    f.write('\\n'.join(docs))\n",
    "del contents\n",
    "\n",
    "f.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce40b19-de13-4e3b-b009-691e5167a37a",
   "metadata": {},
   "source": [
    "# google sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4642fc86-14e3-41f3-8d54-4ce1578e361a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n",
      "CPU times: user 55min 14s, sys: 12.1 s, total: 55min 26s\n",
      "Wall time: 17min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import sentencepiece as spm\n",
    "\n",
    "vocab_size = 35000\n",
    "sp_model_root = './data/tokenizer/'\n",
    "if not os.path.isdir(sp_model_root):\n",
    "    os.mkdir(sp_model_root)\n",
    "sp_model_name = 'blog_sentpiece_%d' % (vocab_size)\n",
    "sp_model_path = os.path.join(sp_model_root, sp_model_name)\n",
    "model_type = 'unigram' # or \"bpe\"\n",
    "character_coverage  = 0.9995\n",
    "user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK],[BOS],[EOS],[UNK0],[UNK1],[UNK2],[UNK3],[UNK4],[UNK5],[UNK6],[UNK7],[UNK8],[UNK9],[unused0],[unused1],[unused2],[unused3],[unused4],[unused5],[unused6],[unused7],[unused8],[unused9],[unused10],[unused11],[unused12],[unused13],[unused14],[unused15],[unused16],[unused17],[unused18],[unused19],[unused20],[unused21],[unused22],[unused23],[unused24],[unused25],[unused26],[unused27],[unused28],[unused29],[unused30],[unused31],[unused32],[unused33],[unused34],[unused35],[unused36],[unused37],[unused38],[unused39],[unused40],[unused41],[unused42],[unused43],[unused44],[unused45],[unused46],[unused47],[unused48],[unused49],[unused50],[unused51],[unused52],[unused53],[unused54],[unused55],[unused56],[unused57],[unused58],[unused59],[unused60],[unused61],[unused62],[unused63],[unused64],[unused65],[unused66],[unused67],[unused68],[unused69],[unused70],[unused71],[unused72],[unused73],[unused74],[unused75],[unused76],[unused77],[unused78],[unused79],[unused80],[unused81],[unused82],[unused83],[unused84],[unused85],[unused86],[unused87],[unused88],[unused89],[unused90],[unused91],[unused92],[unused93],[unused94],[unused95],[unused96],[unused97],[unused98],[unused99]'\n",
    "\n",
    "input_argument = '--input=%s --model_prefix=%s --vocab_size=%s --user_defined_symbols=%s --model_type=%s --character_coverage=%s'\n",
    "cmd = input_argument%(input_f, sp_model_path, vocab_size,user_defined_symbols, model_type, character_coverage)\n",
    "\n",
    "spm.SentencePieceTrainer.Train(cmd)\n",
    "print('train done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "002a1e67-5c1e-49a3-91f0-9d6f3ea2cc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1286, 23664, 9404, 13793, 23982, 121, 17852, 122, 404, 18397, 150, 1138, 18044, 3950, 122, 9054, 188, 5800, 456]\n",
      "['▁사장님', '▁추천메뉴', '▁복숭아', '요거트', '복숭아', '▁', '쨈', '도', '▁완전', '▁상콤', '하고', '▁맛있고', '▁겉에', '▁과자', '도', '▁바삭바삭', '해서', '▁식감도', '▁좋았어요']\n",
      "사장님 추천메뉴 복숭아요거트복숭아 쨈도 완전 상콤하고 맛있고 겉에 과자도 바삭바삭해서 식감도 좋았어요\n",
      "사장님 추천메뉴 복숭아요거트복숭아 쨈도 완전 상콤하고 맛있고 겉에 과자도 바삭바삭해서 식감도 좋았어요\n"
     ]
    }
   ],
   "source": [
    "## check\n",
    "import sentencepiece as spm\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('{}.model'.format(sp_model_path))\n",
    "\n",
    "test_txt = \"사장님 추천메뉴 복숭아요거트복숭아 쨈도 완전 상콤하고 맛있고 겉에 과자도 바삭바삭해서 식감도 좋았어요\"\n",
    "tokens = sp.encode_as_pieces(test_txt)\n",
    "ids = sp.encode_as_ids(test_txt)\n",
    "\n",
    "print(ids)\n",
    "print(tokens)\n",
    "\n",
    "tokens = sp.decode_pieces(tokens)\n",
    "ids = sp.decode_ids(ids)\n",
    "\n",
    "print(ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b57a6ac-3b0e-4a77-82ae-b2529e59e25e",
   "metadata": {},
   "source": [
    "# Huggingface tokenizer_bertwordpiece\n",
    "- Rust로 구현되어 있어 빠르다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a3f9247-5aea-47d3-a9ff-da6f5e157dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.11.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
      "     |████████████████████████████████| 6.8 MB 16.5 MB/s            \n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.11.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16d151d9-9602-4377-95e8-e95c4ff13ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3586068\n",
      "['까막바위', '강원도 동해시 묵호동동해시 까막바위 일출 해맞이 해돋이 겨울바다 한파', '까막바위에서~까막바위']\n",
      "CPU times: user 1.48 s, sys: 512 ms, total: 1.99 s\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_f = \"./data/for_tokenizer_corpus.txt\"\n",
    "with open(input_f, 'r') as f:\n",
    "    data = f.read().splitlines()\n",
    "print(len(data))\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0fccb1-5234-4fef-8e60-55ab3a9abf42",
   "metadata": {},
   "source": [
    "wordpiece 사용시 mecab으로 토큰 후 wordpiece 훈련하는 것이 더 적합할 수 있따."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9acf3e64-20e3-404c-8b23-d1bd44987044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['까막', '바위'], ['강원도', '동해시', '묵호동', '동해시', '까막', '바위', '일출', '해맞이', '해돋이', '겨울', '바다', '한파'], ['까막', '바위', '에서', '~', '까막', '바위']]\n",
      "3586068\n",
      "CPU times: user 4min 42s, sys: 2.52 s, total: 4min 44s\n",
      "Wall time: 4min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from konlpy.tag import Mecab\n",
    "m = Mecab()\n",
    "\n",
    "for_generation = False # or normal\n",
    "\n",
    "if for_generation:\n",
    "    # 1: '어릴때' -> '어릴, ##때' for generation model\n",
    "    total_morph=[]\n",
    "    for sentence in data:\n",
    "        morph_sentence= []\n",
    "        count = 0\n",
    "        for token_mecab in m.morphs(sentence):\n",
    "            token_mecab_save = token_mecab\n",
    "            if count > 0:\n",
    "                token_mecab_save = \"##\" + token_mecab_save\n",
    "                morph_sentence.append(token_mecab_save)\n",
    "            else:\n",
    "                morph_sentence.append(token_mecab_save)\n",
    "                count += 1\n",
    "        total_morph.append(morph_sentence)\n",
    "\n",
    "else:\n",
    "    # 2: '어릴때' -> '어릴, 때'   for normal case\n",
    "    total_morph=[]\n",
    "    for sentence in data:\n",
    "        morph_sentence= m.morphs(sentence)\n",
    "        total_morph.append(morph_sentence)\n",
    "                        \n",
    "print(total_morph[:3])\n",
    "print(len(total_morph))\n",
    "\n",
    "with open('./data/for_tokenizer_corpus_mecab.txt', 'w', encoding='utf-8') as f:\n",
    "    for line in total_morph:\n",
    "        f.write(' '.join(line)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c530ed-ea66-4edd-aa9b-af17660b4e51",
   "metadata": {},
   "source": [
    "- Dummy token -> [unused], [UNK] 설정\n",
    "    - [BOS] 문장 시작 [EOS] 문장 끝\n",
    "    - 특히 도메인 특화 task 수행시 반드시 도메인 토큰을 따로 선언"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a322a-0ce3-4bc0-a4cd-ba6c4b088b5f",
   "metadata": {},
   "source": [
    "- CharBPETokenizer: The original BPE\n",
    "- ByteLevelBPETokenizer: The byte level version of the BPE\n",
    "- SentencePieceBPETokenizer: A BPE implementation compatible with the one used by SentencePiece\n",
    "- BertWordPieceTokenizer: The famous Bert tokenizer, using WordPiece\n",
    "<br>\n",
    "- 한국어는 strip_accents = False로 해줘야 한다\n",
    "    - 만약 True일 시 나는 -> 'ㄴ','ㅏ','ㄴ','ㅡ','ㄴ' 로 쪼개져서 처리된다\n",
    "    - 학습시 False했으므로 load할 때도 False를 꼭 확인\n",
    "- BertWordPieceTokenizer로 학습시킬 땐 lower_case=False 시 strip_accent=False로 지정.<br> huggingface transformer에서 tokenzizer를 load할 때도 strip_accent=False를 꼭 지정해야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993bc950-5839-48be-8810-ccd26cbb0ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertWordPieceTokenizer\n",
      "CPU times: user 5.59 ms, sys: 91 µs, total: 5.68 ms\n",
      "Wall time: 28.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "from tokenizers import BertWordPieceTokenizer, SentencePieceBPETokenizer, CharBPETokenizer, ByteLevelBPETokenizer\n",
    "\n",
    "how_to_tokenize = BertWordPieceTokenizer\n",
    "\n",
    "if str(how_to_tokenize) == str(BertWordPieceTokenizer):\n",
    "    print('BertWordPieceTokenizer')\n",
    "    tokenizer = BertWordPieceTokenizer(strip_accents=False,  # Must be False if cased model\n",
    "                                       lowercase=False)\n",
    "elif str(how_to_tokenize) == str(SentencePieceBPETokenizer):\n",
    "    print('SentencePieceBPETokenizer')\n",
    "    tokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "elif str(how_to_tokenize) == str(CharBPETokenizer):\n",
    "    print('CharBPETokenizer')\n",
    "    tokenizer = CharBPETokenizer()\n",
    "    \n",
    "elif str(how_to_tokenize) == str(ByteLevelBPETokenizer):\n",
    "    print('ByteLevelBPETokenizer')\n",
    "    tokenizer = ByteLevelBPETokenizer()\n",
    "       \n",
    "else:\n",
    "    assert('select right tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4d3f2-2df5-4015-91f3-a00cfbba97f6",
   "metadata": {},
   "source": [
    "- huggingface tokinizer params\n",
    "    - min_frequency : merge를 수행할 최소 빈도수, 5로 설정 시 5회 이상 등장한 pair만 수행한다\n",
    "    - vocab_size: 만들고자 하는 vocab의 size, 보통 '32000' 정도가 좋다고 알려져 있다\n",
    "    - show_progress : 학습 진행과정 show\n",
    "    - special_tokens : Tokenizer에 추가하고 싶은 special token 지정\n",
    "    - limit_alphabet : merge 수행 전 initial tokens이 유지되는 숫자 제한. ByteLevelBPETokenizer 학습시엔 주석처리 필요\n",
    "    - initial_alphabet : 꼭 포함됐으면 하는 initial alphabet, 이곳에 설정한 token은 학습되지 않고 그대로 포함되도록 설정된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0890f6e-ea19-4745-ab6c-e4d7989caa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train complete\n",
      "사장님 추천메뉴 복숭아요거트복숭아 쨈도 완전 상콤하고 맛있고 겉에 과자도 바삭바삭해서 식감도 좋았어요\n",
      "=>idx   : [6604, 4385, 6530, 12704, 10138, 4262, 4314, 4189, 4201, 10020, 3025, 4237, 6623, 2001, 4725, 17080, 6474, 4213, 142, 4444, 8155, 4237, 8556, 6991, 6885, 4237, 2871, 5651, 6682]\n",
      "=>tokens: ['사장', '##님', '추천', '##메뉴', '복숭아', '##요', '##거', '##트', '##복', '##숭아', '쨈', '##도', '완전', '상', '##콤', '##하고', '맛있', '##고', '겉', '##에', '과자', '##도', '바삭바삭', '##해서', '식감', '##도', '좋', '##았', '##어요']\n",
      "=>offset: [(0, 2), (2, 3), (4, 6), (6, 8), (9, 12), (12, 13), (13, 14), (14, 15), (15, 16), (16, 18), (19, 20), (20, 21), (22, 24), (25, 26), (26, 27), (27, 29), (30, 32), (32, 33), (34, 35), (35, 36), (37, 39), (39, 40), (41, 45), (45, 47), (48, 50), (50, 51), (52, 53), (53, 54), (54, 56)]\n",
      "=>decode: 사장님 추천메뉴 복숭아요거트복숭아 쨈도 완전 상콤하고 맛있고 겉에 과자도 바삭바삭해서 식감도 좋았어요\n",
      "\n",
      "I want to go my hometown\n",
      "=>idx   : [31, 71, 22082, 13757, 12466, 8192, 8854, 4200, 20770]\n",
      "=>tokens: ['I', 'w', '##ant', 'to', 'go', 'my', 'home', '##t', '##own']\n",
      "=>offset: [(0, 1), (2, 3), (3, 6), (7, 9), (10, 12), (13, 15), (16, 20), (20, 21), (21, 24)]\n",
      "=>decode: I want to go my hometown\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hf_tokenizer/vocab.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file   = [\"./data/for_tokenizer_corpus_mecab.txt\"]  # data path\n",
    "vocab_size    = 30000 # 35000 #######\n",
    "limit_alphabet= 5000 # 6000 #######\n",
    "output_path   = 'hugging_%d'%(vocab_size)\n",
    "min_frequency = 2 # 5\n",
    "\n",
    "# train\n",
    "tokenizer.train(files=corpus_file,\n",
    "               vocab_size=vocab_size,\n",
    "               min_frequency=min_frequency,\n",
    "               limit_alphabet=limit_alphabet,\n",
    "               show_progress=True)\n",
    "print('train complete')\n",
    "\n",
    "test_txt = \"사장님 추천메뉴 복숭아요거트복숭아 쨈도 완전 상콤하고 맛있고 겉에 과자도 바삭바삭해서 식감도 좋았어요\"\n",
    "sentence = test_txt\n",
    "output = tokenizer.encode(sentence)\n",
    "print(sentence)\n",
    "print('=>idx   : %s'%output.ids)\n",
    "print('=>tokens: %s'%output.tokens)\n",
    "print('=>offset: %s'%output.offsets)\n",
    "print('=>decode: %s\\n'%tokenizer.decode(output.ids))\n",
    "\n",
    "sentence = 'I want to go my hometown'\n",
    "output = tokenizer.encode(sentence)\n",
    "print(sentence)\n",
    "print('=>idx   : %s'%output.ids)\n",
    "print('=>tokens: %s'%output.tokens)\n",
    "print('=>offset: %s'%output.offsets)\n",
    "print('=>decode: %s\\n'%tokenizer.decode(output.ids))\n",
    "\n",
    "# save tokenizer\n",
    "hf_model_path='hf_tokenizer'\n",
    "if not os.path.isdir(hf_model_path):\n",
    "    os.mkdir(hf_model_path)\n",
    "tokenizer.save_model(hf_model_path)  # vocab.txt 파일 한개가 만들어진다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5117d870-6e57-48f9-b942-cf7e1fd4ef28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27eccee-3718-4f05-bdc1-5f3322395e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file hf_tokenizer/config.json not found\n",
      "file hf_tokenizer/config.json not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voca size : 30000\n",
      "Token (str) : ['[CLS]', '과자', '##가', '완전', '바삭', '[SEP]']\n",
      "Token (int) : [[2, 8155, 4391, 6623, 7016, 3]]\n",
      "Token (attn_mask) : [[1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# transformer usage check\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "loaded_tokenizer = BertTokenizerFast.from_pretrained(hf_model_path, strip_accents=False, lowercase=False)\n",
    "print('voca size : %d' % loaded_tokenizer.vocab_size)\n",
    "tokenized_input_for_torch = loaded_tokenizer('과자가 완전 바삭', return_tensors='pt')\n",
    "\n",
    "print('Token (str) : {}'.format([loaded_tokenizer.convert_ids_to_tokens(s) for s in tokenized_input_for_torch['input_ids'].tolist()[0]]))\n",
    "print('Token (int) : {}'.format([tokenized_input_for_torch['input_ids'].tolist()[0]]))\n",
    "print('Token (attn_mask) : {}'.format([tokenized_input_for_torch['attention_mask'].tolist()[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "539aa00b-dcb3-4ff6-9000-98f8860f7a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Co', 27554),\n",
       " ('할리', 21975),\n",
       " ('수목', 9956),\n",
       " ('남이섬', 17773),\n",
       " ('버티', 16050),\n",
       " ('귑', 271),\n",
       " ('대한민국', 9415),\n",
       " ('쉑쉑', 12522),\n",
       " ('녹', 639),\n",
       " ('방면', 12326)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(list(loaded_tokenizer.get_vocab().items()), 10) # 랜덤으로 10개 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77bdfecb-047c-420c-a0d1-afb8810b2546",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_defined_symbols = ['[PAD]', '[UNK]', '[UNK0]','[UNK1]','[UNK2]','[UNK3]','[UNK4]','[UNK5]','[UNK6]','[UNK7]','[UNK8]','[UNK9]', '[CLS]', '[SEP]', '[MASK]', '[BOS]','[EOS]']\n",
    "unused_token_num = 200\n",
    "unused_list = ['[unused{}]'.format(n) for n in range(unused_token_num)]\n",
    "user_defined_symbols = user_defined_symbols + unused_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7921dbe3-c1a5-4be4-b8b4-1b841dc9d77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_tokenizer.all_special_tokens # 사용자 정의 심볼 train시 등록해도 등록X. load해서 사용시 따로 추가해줘야함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c89308b5-d6c4-45f2-8315-9665e82232eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[unused190]',\n",
       " '[unused191]',\n",
       " '[unused192]',\n",
       " '[unused193]',\n",
       " '[unused194]',\n",
       " '[unused195]',\n",
       " '[unused196]',\n",
       " '[unused197]',\n",
       " '[unused198]',\n",
       " '[unused199]']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user define symbol 추가하는 방법\n",
    "special_token_dic = {'additional_special_tokens': user_defined_symbols}\n",
    "loaded_tokenizer.add_special_tokens(special_token_dic)\n",
    "\n",
    "loaded_tokenizer.all_special_tokens[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10b8db3b-f88f-4202-af9b-eb2136e70ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hf_tokenizer_special/tokenizer_config.json',\n",
       " 'hf_tokenizer_special/special_tokens_map.json',\n",
       " 'hf_tokenizer_special/vocab.txt',\n",
       " 'hf_tokenizer_special/added_tokens.json',\n",
       " 'hf_tokenizer_special/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special token 추가 후 저장\n",
    "loaded_tokenizer.save_pretrained(hf_model_path+'_special')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1807242c-3270-4707-bd68-0d60943beb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check special tokens : ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]', '[BOS]', '[EOS]', '[unused0]', '[unused1]', '[unused2]']\n",
      "vocab size : 30000\n",
      "Tokens (str)      : ['[CLS]', '완전', '상', '##콤', '##하고', '맛있', '##고', '겉', '##에', '과자', '##도', '바삭바삭', '##해서', '식감', '##도', '좋', '##았', '##어요', '[SEP]']\n",
      "Tokens (int)      : [2, 6623, 2001, 4725, 17080, 6474, 4213, 142, 4444, 8155, 4237, 8556, 6991, 6885, 4237, 2871, 5651, 6682, 3]\n",
      "Tokens (attn_mask): [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check special tokens\n",
    "from transformers import BertTokenizerFast\n",
    "tokenizer_check = BertTokenizerFast.from_pretrained('hf_tokenizer'+'_special')\n",
    "\n",
    "print('check special tokens : %s'%tokenizer_check.all_special_tokens[:20])\n",
    "\n",
    "print('vocab size : %d' % tokenizer_check.vocab_size)\n",
    "tokenized_input_for_pytorch = tokenizer_check(\"완전 상콤하고 맛있고 겉에 과자도 바삭바삭해서 식감도 좋았어요\", return_tensors=\"pt\")\n",
    "\n",
    "print(\"Tokens (str)      : {}\".format([tokenizer_check.convert_ids_to_tokens(s) for s in tokenized_input_for_pytorch['input_ids'].tolist()[0]]))\n",
    "print(\"Tokens (int)      : {}\".format(tokenized_input_for_pytorch['input_ids'].tolist()[0]))\n",
    "print(\"Tokens (attn_mask): {}\\n\".format(tokenized_input_for_pytorch['attention_mask'].tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35629cfd-2e17-4793-bd71-e0873204d95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final layer output shape : torch.Size([1, 20, 768])\n"
     ]
    }
   ],
   "source": [
    "# torch에 input으로 잘 들어가는지 확인\n",
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-cased') # test용 모델\n",
    "input_sent = tokenizer_check(\"완전 상콤하고 맛있고 겉에 과자도 바삭바삭해서 식감도 좋았어요.\", return_tensors=\"pt\") # pt: pytorch, tf: tensorflow\n",
    "ouput_sent = model(**input_sent)\n",
    "print('final layer output shape : %s' % (ouput_sent['last_hidden_state'].shape,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
