{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef71e37-a3cb-455f-8406-05f916c09e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a50ebcd-22f9-49b6-8491-27684acd8785",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/model_doc/bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd93ca8-8c5d-4527-938d-26102e48ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer\n",
    "from transformers import BertConfig, BertForPreTraining, BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast(\n",
    "    vocab_file = './hf_tokenizer_special/vocab.txt',\n",
    "    max_len = 128,\n",
    "    do_lower_case=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6266b85d-5bf2-456f-9eb0-4125cbdf6fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '은', '[UNK]', 'MA', '##S', '##K', '[UNK]', '엄청', '맛있', '##었', '##고', '촉촉', '##하고', '바삭', '##했', '##어요', '.']\n",
      "['[UNK]', '은', '[MASK]', '엄청', '맛있', '##었', '##고', '촉촉', '##하고', '바삭', '##했', '##어요', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize('曔은 [MASK] 엄청 맛있었고 촉촉하고 바삭했어요.'))\n",
    "tokenizer.add_special_tokens({'mask_token':'[MASK]'})\n",
    "print(tokenizer.tokenize('曔은 [MASK] 엄청 맛있었고 촉촉하고 바삭했어요.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee41091c-3c97-4166-a1ee-0e0700dda57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109705010"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig(\n",
    "    vocab_size = 30000, #####\n",
    "    # hidden_size=512,\n",
    "    # num_hidden_layers=12,\n",
    "    # num_attention_heads=8,\n",
    "    # intermediate_size=3072, # transformer 내의 feed-forward network dimension size\n",
    "    # hidden_act='gelu',\n",
    "    # hidden_dropout_prob=0.1,\n",
    "    # attention_probs_dropout_prob=0.1,\n",
    "    max_position_embedding=128, # limit num of tokens from sentence\n",
    "    # type_vocab_size=2,\n",
    "    # pad_token_id=0,\n",
    "    # position_embedding_type='absolute',\n",
    ")\n",
    "\n",
    "model = BertForPreTraining(config=config)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b58f1e-0be7-4f2a-b798-ef5a512da104",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from filelock import FileLock\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers.utils import logging\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "338b6cf3-8166-4773-98c2-7709c7043f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making corpus \n",
    "# \\n : separate sentence. \\n\\n : separate document.\n",
    "from dbConn.mongo_conn import config\n",
    "\n",
    "conn = config()\n",
    "col = conn[\"travel_ai\"].blog_contents\n",
    "contents = col.find({\"num_docs\": {\"$gt\": 1}}, {\"cleaned_content\": 1})\n",
    "input_f = \"./data/for_pretrain_corpus.txt\"\n",
    "\n",
    "f = open(input_f, \"w\")\n",
    "for cont in contents:\n",
    "    docs = [c for c in cont['cleaned_content']]\n",
    "    f.write('\\n'.join(docs))\n",
    "    f.write('\\n')\n",
    "del contents\n",
    "\n",
    "f.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8e1bc26-74f0-4c87-8319-ab4a809c160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT task 1 : NSP, Next Sentence Prediction\n",
    "\n",
    "class TextDatasetForNextSentencePrediction(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        file_path: str,\n",
    "        block_size: int,\n",
    "        overwrite_cache=False,\n",
    "        short_seq_probability=0.1,\n",
    "        nsp_probability=0.5,\n",
    "    ):\n",
    "        # training data caching\n",
    "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
    "\n",
    "        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=True)\n",
    "        self.short_seq_probability = short_seq_probability\n",
    "        self.nsp_probability = nsp_probability\n",
    "\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            directory,\n",
    "            \"cached_nsp_{}_{}_{}\".format(\n",
    "                tokenizer.__class__.__name__,\n",
    "                str(block_size),\n",
    "                filename,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "\n",
    "        # Input file format:\n",
    "        # (1) One sentence per line. These should ideally be actual sentences, not\n",
    "        # entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "        # sentence boundaries for the \"next sentence prediction\" task).\n",
    "        # (2) Blank lines between documents. Document boundaries are needed so\n",
    "        # that the \"next sentence prediction\" task doesn't span between documents.\n",
    "        #\n",
    "        # Example:\n",
    "        # I am very happy.\n",
    "        # Here is the second sentence.\n",
    "        #\n",
    "        # A new document.\n",
    "\n",
    "        with FileLock(lock_path):\n",
    "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    self.examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "            else: # no cached data\n",
    "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
    "                self.documents = [[]] # training per document\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    while True:\n",
    "                        line = f.readline()\n",
    "                        if not line:\n",
    "                            break\n",
    "                        line = line.strip()\n",
    "\n",
    "                        # \\n\\n -> documnet\n",
    "                        if not line and len(self.documents[-1]) != 0:\n",
    "                            self.documents.append([])\n",
    "                        tokens = tokenizer.tokenize(line)\n",
    "                        tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                        if tokens:\n",
    "                            self.documents[-1].append(tokens)\n",
    "                logger.info(f\"Creating examples from {len(self.documents)} documents.\")\n",
    "                self.examples = []\n",
    "                # transform for training data\n",
    "                for doc_index, document in enumerate(self.documents):\n",
    "                    self.create_examples_from_document(document, doc_index) # 함수로 가봅시다.\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def create_examples_from_document(self, document: List[List[int]], doc_index: int):\n",
    "        \"\"\"Creates examples for a single document.\"\"\"\n",
    "        # size - 2 because of [CLS], [SEP] token, so limit max_position_embedding - 2 each sentence\n",
    "        max_num_tokens = self.block_size - self.tokenizer.num_special_tokens_to_add(pair=True)\n",
    "\n",
    "        # We *usually* want to fill up the entire sequence since we are padding\n",
    "        # to `block_size` anyways, so short sequences are generally wasted\n",
    "        # computation. However, we *sometimes*\n",
    "        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "        # sequences to minimize the mismatch between pretraining and fine-tuning.\n",
    "        # The `target_seq_length` is just a rough target however, whereas\n",
    "        # `block_size` is a hard limit.\n",
    "\n",
    "        target_seq_length = max_num_tokens\n",
    "        if random.random() < self.short_seq_probability: # short_seq_probability 2 ~ (max_position_embedding - 2) 사이의 랜덤 학습 데이터 생성\n",
    "            target_seq_length = random.randint(2, max_num_tokens)\n",
    "\n",
    "        current_chunk = []  # a buffer stored current working segments\n",
    "        current_length = 0\n",
    "        i = 0\n",
    "\n",
    "        # sentence_1[SEP]sentence_2 이 기본이나, (max_position_embedding - 2)길이의 토큰을 채울 수 있도록\n",
    "        # sentence_1+sentence_2[SEP]sentence_3+sentence_4 형태로 만들어질 수 있음\n",
    "        while i < len(document):\n",
    "            segment = document[i]\n",
    "            current_chunk.append(segment)\n",
    "            current_length += len(segment)\n",
    "            if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "                if current_chunk:\n",
    "                    # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                    # (first) sentence.\n",
    "                    a_end = 1\n",
    "                    # sentence_1+sentence_2 가 이루어졌을 때, 길이를 random하게 자름\n",
    "                    if len(current_chunk) >= 2:\n",
    "                        a_end = random.randint(1, len(current_chunk) - 1)\n",
    "                    tokens_a = []\n",
    "                    for j in range(a_end):\n",
    "                        tokens_a.extend(current_chunk[j])\n",
    "                    # [SEP] 뒷부분 문장 처리\n",
    "                    tokens_b = []\n",
    "                    # 50%의 확률로 랜덤하게 다른 문장을 선택하거나, 다음 문장을 학습데이터로 생성.\n",
    "                    if len(current_chunk) == 1 or random.random() < self.nsp_probability:\n",
    "                        is_random_next = True\n",
    "                        target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "                        # This should rarely go for more than one iteration for large\n",
    "                        # corpora. However, just to be careful, we try to make sure that\n",
    "                        # the random document is not the same as the document\n",
    "                        # we're processing.\n",
    "                        for _ in range(10):\n",
    "                            random_document_index = random.randint(0, len(self.documents) - 1)\n",
    "                            if random_document_index != doc_index:\n",
    "                                break\n",
    "                        # ransom sampling\n",
    "                        random_document = self.documents[random_document_index]\n",
    "                        random_start = random.randint(0, len(random_document) - 1)\n",
    "                        for j in range(random_start, len(random_document)):\n",
    "                            tokens_b.extend(random_document[j])\n",
    "                            if len(tokens_b) >= target_b_length:\n",
    "                                break\n",
    "                        # We didn't actually use these segments so we \"put them back\" so\n",
    "                        # they don't go to waste.\n",
    "                        num_unused_segments = len(current_chunk) - a_end\n",
    "                        i -= num_unused_segments\n",
    "                    # Actual next\n",
    "                    else:\n",
    "                        is_random_next = False\n",
    "                        for j in range(a_end, len(current_chunk)):\n",
    "                            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                    # over (max_position_embedding - 2), remove segmentA or segmentB randomly\n",
    "                    def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n",
    "                        \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "                        while True:\n",
    "                            total_length = len(tokens_a) + len(tokens_b)\n",
    "                            if total_length <= max_num_tokens:\n",
    "                                break\n",
    "                            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "                            assert len(trunc_tokens) >= 1\n",
    "                            # We want to sometimes truncate from the front and sometimes from the\n",
    "                            # back to add more randomness and avoid biases.\n",
    "                            if random.random() < 0.5:\n",
    "                                del trunc_tokens[0]\n",
    "                            else:\n",
    "                                trunc_tokens.pop()\n",
    "\n",
    "                    truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n",
    "\n",
    "                    assert len(tokens_a) >= 1\n",
    "                    assert len(tokens_b) >= 1\n",
    "\n",
    "                    # add special tokens\n",
    "                    input_ids = self.tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)\n",
    "                    # add token type ids, 0 for sentence a, 1 for sentence b\n",
    "                    token_type_ids = self.tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)\n",
    "                    \n",
    "                    # completed making NSP dataset\n",
    "                    example = {\n",
    "                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                        \"next_sentence_label\": torch.tensor(1 if is_random_next else 0, dtype=torch.long),\n",
    "                    }\n",
    "\n",
    "                    self.examples.append(example)\n",
    "\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d652c10-932e-41ef-bf6e-ec7404a885d2",
   "metadata": {},
   "source": [
    "### Bert Architecture\n",
    "- BASE : transformer block(L):12, hidden layer size(H):768, Attention head(A):12\n",
    "- LARGE : transformer block(L):24, hidden layer size(H):1024, Attention head(A):16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9933531a-d349-4a9f-8ccc-6679e4ab3050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (139 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 8s, sys: 5.78 s, total: 8min 14s\n",
      "Wall time: 8min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = TextDatasetForNextSentencePrediction(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./data/for_pretrain_corpus.txt\",\n",
    "    block_size=128,\n",
    "    overwrite_cache=False,\n",
    "    short_seq_probability=0.1,\n",
    "    nsp_probability=0.5,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(    # task 2: no need extra implement for MSM [MASK]\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd1e2d6-5ddf-4b24-8743-3227ec307e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2, 19256,  8432,  8803, 12993, 16017,  7147, 10416, 20764, 19256,\n",
      "         8432, 27446,    75, 19256,  8432,  9415,  9798, 19256,  8432,  4263,\n",
      "         4767,  7147, 10416, 19256,  8432,  8803,  9798, 28536, 19256,  8432,\n",
      "         4650,  7027, 27446,  8754,  7355,  4240,  8803,  7355,  4263,  4767,\n",
      "         7355, 10416, 19256,  8432,  7007,  4274,  4206,  4214,  4266,  6650,\n",
      "         2234,  4477,  4382,  6843, 10483,  8159,  4240,  6959,  8637,  7443,\n",
      "         4633,  8730,  4444,  1710, 17246,  6925,  9751,     3,  7010,  8159,\n",
      "        19752,  4444,  6552,  4494,  8996,  4330,  4328,  6926,  4478,  6703,\n",
      "        10483,     9,  6952,  4444,  8996,  4045, 16041,  4391,  2755,  4291,\n",
      "         7693,  4237,  7738,  2136,  2755, 16967,     9,  6644,     7,  6490,\n",
      "         9859,  2244,  4229,  4291,     5, 10412,  5273,  4144,  4277,  4942,\n",
      "         6551,  8996,  4330,  4900,  8996,  4330,  4328,  6926,  7010,  8159,\n",
      "         8240,  6644, 13087,  8292,     3]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1]), 'next_sentence_label': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "# check dataset\n",
    "for example in dataset.examples[0:1]:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11223083-07a6-436b-a0c3-ce176938dc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2, 19256,     4,  ..., 13087,  8292,     3],\n",
      "        [    2,  7355,  4240,  ...,  4196,     4,     3],\n",
      "        [    2,  7010, 18911,  ...,  6593,  6483,     3],\n",
      "        ...,\n",
      "        [    2,  4023,  4330,  ...,     4, 10795,     3],\n",
      "        [    2,  8210,     4,  ..., 11215, 11652,     3],\n",
      "        [    2,  7160, 28685,  ...,  4494,  4371,     3]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1],\n",
      "        [0, 0, 0,  ..., 1, 1, 1]]), 'next_sentence_label': tensor([0, 1, 1,  ..., 0, 1, 1]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]), 'labels': tensor([[-100, -100, 8432,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 6494, -100],\n",
      "        [-100, -100, -100,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [-100, -100, -100,  ..., 9899, -100, -100],\n",
      "        [-100, -100, 4444,  ..., -100, -100, -100],\n",
      "        [-100, -100, -100,  ..., -100, 4371, -100]])}\n"
     ]
    }
   ],
   "source": [
    "# check MSM data collator\n",
    "print(data_collator(dataset.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eb2add9-c8d7-448e-bb47-2567dcb9e896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2, 19256,  8432,  8803, 12993, 16017,  7147, 10416, 20764, 19256,\n",
      "         8432, 27446,     4,     4,  8432,  9415,  9798, 19256,  8432,  4263,\n",
      "         4767,  7147, 10416, 19256,  8432,  8803,  9798, 28536, 19256,  8432,\n",
      "         4650,  7027, 27446,  8754,     4,  4240,  8803,  7355,  4263,  4767,\n",
      "         7355, 10416, 19256,  8432,  7007,  4274,  4206,  4214,  4266,  6650,\n",
      "         2234,  4477,     4,  6843, 10483,  8159,     4, 16082,  8637,  7443,\n",
      "            4,  8730,     4,  1710, 17246,  6925,  9751,     3,  7010,  8159,\n",
      "        19752,  4444,  6552,  4494,  8996,  4330,  4328,  6926,     4,  6703,\n",
      "        10483,     9,  6952,  4444,  8996,  4045, 16041,  4391,     4,  4291,\n",
      "            4,  4237,     4,  2136,  2755,     4,     9,  6644,     7,  6490,\n",
      "         9859,  2244,  4229,     4,     5, 10412,  5273,  4144,  4277,  4942,\n",
      "         6551,  8996,     4,  4900,  8996,     4,  4328,  6926,  7010,  8159,\n",
      "         8240,  6644, 13087,  8292,     3])\n"
     ]
    }
   ],
   "source": [
    "print(data_collator(dataset.examples)['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d30a8036-528d-4c75-9f7e-b7aab384b5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 까막바위 일출 해맞이 해돋이 겨울바다 한파 까막바위에서 ~ 까막바위 대한민국 동해시 까막바위일출 겨울바다 까막바위 일출 동해시 묵호진동 까막바위옆 숙소에서 바라본 동해의 일출 동해일출 [MASK]바다 까막바위 자연은있는그대로 심 모였당 사랑합니다 고성의 모든 여행지 정보를 [MASK]에 [MASK]려면 아래 [MASK] [SEP] 강원도 고성 죽왕면에 위치한 가진 [MASK]늠욕장을 소개합니다. [MASK]에 가진 회센터 [MASK] 있어 물회도 드실 수 있습니다. 바다 [MASK] 정말 보고 싶었어! 양평댁 히나 [MASK] 여행 가진해변 가진해수욕장 강원도 [MASK] [MASK] 바다여행 휴식 [SEP]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(data_collator(dataset.examples)['input_ids'][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c88ae42d-e2df-4b2c-9fdb-b8bd7b8ec638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start pretrain\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='model_output',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32, # 32\n",
    "    save_steps=10000, # model save each step\n",
    "    save_total_limit=2, # only save last 2 models\n",
    "    logging_steps=10000\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46d7ed61-8e02-45f7-b13d-4ebda8bd55ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 930019\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 290640\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='290640' max='290640' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [290640/290640 15:17:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>7.256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>5.500500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>4.345400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>3.880900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>3.606800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>3.416800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>3.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>3.152500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>3.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>2.966700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>2.912300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>2.847300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>2.784200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>2.756200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>2.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>2.656900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>2.636800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>2.588400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190000</td>\n",
       "      <td>2.555800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>2.538200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210000</td>\n",
       "      <td>2.502600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220000</td>\n",
       "      <td>2.478600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230000</td>\n",
       "      <td>2.464900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240000</td>\n",
       "      <td>2.429700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250000</td>\n",
       "      <td>2.413200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260000</td>\n",
       "      <td>2.403500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270000</td>\n",
       "      <td>2.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280000</td>\n",
       "      <td>2.367400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290000</td>\n",
       "      <td>2.362900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model_output/checkpoint-10000\n",
      "Configuration saved in model_output/checkpoint-10000/config.json\n",
      "Model weights saved in model_output/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to model_output/checkpoint-20000\n",
      "Configuration saved in model_output/checkpoint-20000/config.json\n",
      "Model weights saved in model_output/checkpoint-20000/pytorch_model.bin\n",
      "Saving model checkpoint to model_output/checkpoint-30000\n",
      "Configuration saved in model_output/checkpoint-30000/config.json\n",
      "Model weights saved in model_output/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-40000\n",
      "Configuration saved in model_output/checkpoint-40000/config.json\n",
      "Model weights saved in model_output/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-50000\n",
      "Configuration saved in model_output/checkpoint-50000/config.json\n",
      "Model weights saved in model_output/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-60000\n",
      "Configuration saved in model_output/checkpoint-60000/config.json\n",
      "Model weights saved in model_output/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-40000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-70000\n",
      "Configuration saved in model_output/checkpoint-70000/config.json\n",
      "Model weights saved in model_output/checkpoint-70000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-50000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-80000\n",
      "Configuration saved in model_output/checkpoint-80000/config.json\n",
      "Model weights saved in model_output/checkpoint-80000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-60000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-90000\n",
      "Configuration saved in model_output/checkpoint-90000/config.json\n",
      "Model weights saved in model_output/checkpoint-90000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-70000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-100000\n",
      "Configuration saved in model_output/checkpoint-100000/config.json\n",
      "Model weights saved in model_output/checkpoint-100000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-80000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-110000\n",
      "Configuration saved in model_output/checkpoint-110000/config.json\n",
      "Model weights saved in model_output/checkpoint-110000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-90000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-120000\n",
      "Configuration saved in model_output/checkpoint-120000/config.json\n",
      "Model weights saved in model_output/checkpoint-120000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-100000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-130000\n",
      "Configuration saved in model_output/checkpoint-130000/config.json\n",
      "Model weights saved in model_output/checkpoint-130000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-110000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-140000\n",
      "Configuration saved in model_output/checkpoint-140000/config.json\n",
      "Model weights saved in model_output/checkpoint-140000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-120000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-150000\n",
      "Configuration saved in model_output/checkpoint-150000/config.json\n",
      "Model weights saved in model_output/checkpoint-150000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-130000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-160000\n",
      "Configuration saved in model_output/checkpoint-160000/config.json\n",
      "Model weights saved in model_output/checkpoint-160000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-140000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-170000\n",
      "Configuration saved in model_output/checkpoint-170000/config.json\n",
      "Model weights saved in model_output/checkpoint-170000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-150000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-180000\n",
      "Configuration saved in model_output/checkpoint-180000/config.json\n",
      "Model weights saved in model_output/checkpoint-180000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-160000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-190000\n",
      "Configuration saved in model_output/checkpoint-190000/config.json\n",
      "Model weights saved in model_output/checkpoint-190000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-170000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-200000\n",
      "Configuration saved in model_output/checkpoint-200000/config.json\n",
      "Model weights saved in model_output/checkpoint-200000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-180000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-210000\n",
      "Configuration saved in model_output/checkpoint-210000/config.json\n",
      "Model weights saved in model_output/checkpoint-210000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-190000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-220000\n",
      "Configuration saved in model_output/checkpoint-220000/config.json\n",
      "Model weights saved in model_output/checkpoint-220000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-200000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-230000\n",
      "Configuration saved in model_output/checkpoint-230000/config.json\n",
      "Model weights saved in model_output/checkpoint-230000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-210000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-240000\n",
      "Configuration saved in model_output/checkpoint-240000/config.json\n",
      "Model weights saved in model_output/checkpoint-240000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-220000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-250000\n",
      "Configuration saved in model_output/checkpoint-250000/config.json\n",
      "Model weights saved in model_output/checkpoint-250000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-230000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-260000\n",
      "Configuration saved in model_output/checkpoint-260000/config.json\n",
      "Model weights saved in model_output/checkpoint-260000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-240000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-270000\n",
      "Configuration saved in model_output/checkpoint-270000/config.json\n",
      "Model weights saved in model_output/checkpoint-270000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-250000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-280000\n",
      "Configuration saved in model_output/checkpoint-280000/config.json\n",
      "Model weights saved in model_output/checkpoint-280000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-260000] due to args.save_total_limit\n",
      "Saving model checkpoint to model_output/checkpoint-290000\n",
      "Configuration saved in model_output/checkpoint-290000/config.json\n",
      "Model weights saved in model_output/checkpoint-290000/pytorch_model.bin\n",
      "Deleting older checkpoint [model_output/checkpoint-270000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=290640, training_loss=3.0747561038180287, metrics={'train_runtime': 55063.0656, 'train_samples_per_second': 168.901, 'train_steps_per_second': 5.278, 'total_flos': 6.017473430617651e+17, 'train_loss': 3.0747561038180287, 'epoch': 10.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d8a0fc9-45e0-4289-8eb6-a31418bdcd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./model_output\n",
      "Configuration saved in ./model_output/config.json\n",
      "Model weights saved in ./model_output/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('./model_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "540a25d7-3890-4e00-8d7e-fa275f486c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44400e15-df0a-4031-970e-eeae6709fffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file model_output/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embedding\": 128,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file model_output/pytorch_model.bin\n",
      "Some weights of the model checkpoint at model_output were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at model_output.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "my_model = BertForMaskedLM.from_pretrained('model_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40a482f5-9772-45fa-a680-0057fc259230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘',\n",
       " '카페',\n",
       " '##에',\n",
       " '##가',\n",
       " '##서',\n",
       " '[MASK]',\n",
       " '를',\n",
       " '먹',\n",
       " '##었',\n",
       " '##는데',\n",
       " '##요',\n",
       " ',',\n",
       " '너무',\n",
       " '맛있',\n",
       " '##떠',\n",
       " '##라',\n",
       " '##구요',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('오늘 카페에가서 [MASK]를 먹었는데요, 너무 맛있떠라구요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76cc8ecf-3e3e-4c2c-948e-aac858a67454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.24728845059871674,\n",
       "  'token': 7092,\n",
       "  'token_str': '아메리카노',\n",
       "  'sequence': '[CLS] 오늘 카페에가서 아메리카노 를 먹었는데요, 너무 맛있떠라구요. [SEP]'},\n",
       " {'score': 0.1023724377155304,\n",
       "  'token': 6527,\n",
       "  'token_str': '커피',\n",
       "  'sequence': '[CLS] 오늘 카페에가서 커피 를 먹었는데요, 너무 맛있떠라구요. [SEP]'},\n",
       " {'score': 0.04726902395486832,\n",
       "  'token': 8890,\n",
       "  'token_str': '아아',\n",
       "  'sequence': '[CLS] 오늘 카페에가서 아아 를 먹었는데요, 너무 맛있떠라구요. [SEP]'},\n",
       " {'score': 0.040487710386514664,\n",
       "  'token': 6798,\n",
       "  'token_str': '디저트',\n",
       "  'sequence': '[CLS] 오늘 카페에가서 디저트 를 먹었는데요, 너무 맛있떠라구요. [SEP]'},\n",
       " {'score': 0.038824670016765594,\n",
       "  'token': 10955,\n",
       "  'token_str': '팥빙수',\n",
       "  'sequence': '[CLS] 오늘 카페에가서 팥빙수 를 먹었는데요, 너무 맛있떠라구요. [SEP]'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_fill = pipeline('fill-mask', top_k=5, model=my_model, tokenizer=tokenizer)\n",
    "nlp_fill('오늘 카페에가서 [MASK]를 먹었는데요, 너무 맛있떠라구요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66cd8dfc-b338-42fe-8484-47846fb1c15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.02833535522222519,\n",
       "  'token': 6,\n",
       "  'token_str': ')',\n",
       "  'sequence': '[CLS] ) 를 먹었는데요, 너무 맛있떠라구요. [SEP]'},\n",
       " {'score': 0.025303687900304794,\n",
       "  'token': 8570,\n",
       "  'token_str': '고등어',\n",
       "  'sequence': '[CLS] 고등어 를 먹었는데요, 너무 맛있떠라구요. [SEP]'},\n",
       " {'score': 0.020374711602926254,\n",
       "  'token': 7121,\n",
       "  'token_str': '장어',\n",
       "  'sequence': '[CLS] 장어 를 먹었는데요, 너무 맛있떠라구요. [SEP]'},\n",
       " {'score': 0.020118871703743935,\n",
       "  'token': 7989,\n",
       "  'token_str': '떡갈비',\n",
       "  'sequence': '[CLS] 떡갈비 를 먹었는데요, 너무 맛있떠라구요. [SEP]'},\n",
       " {'score': 0.014281458221375942,\n",
       "  'token': 22,\n",
       "  'token_str': '?',\n",
       "  'sequence': '[CLS]? 를 먹었는데요, 너무 맛있떠라구요. [SEP]'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_fill('[MASK]를 먹었는데요, 너무 맛있떠라구요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b7068-9d7c-4eb0-9418-874992bea68d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
