{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497df280-3b43-416d-92e2-cb878817159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/travel_ai/huggingface_konlpy')\n",
    "\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "from transformers_konlpy import KoNLPyBertTokenizer\n",
    "from tokenizers_konlpy import KoNLPyWordPieceTokenizer\n",
    "from transformers import BasicTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6912ee-8359-4fa9-b353-8925e02ac822",
   "metadata": {},
   "source": [
    "### Single Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839df9e8-7d18-4382-a8f6-dabae4bbe1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KoNLPyBertTokenizer(\n",
    "    konlpy_wordpiece = KoNLPyWordPieceTokenizer(Mecab(), use_tag=True),\n",
    "    vocab_file = '../tokenizer/konlpy_tokenizer/vocab.txt'\n",
    ")\n",
    "basic_tokenizer = BasicTokenizer(do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0cba816-f138-4d22-8c35-c0a153746e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65543688"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "with open('../data/preprocessed_corpus.txt') as f:\n",
    "    cased_lines = f.read().splitlines()\n",
    "len(cased_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb686898-627c-4c34-bfe6-6b0507d12286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(cased_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "725cfc4d-0be3-4400-b761-85205f8c9676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65543688"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = len(cased_lines)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79f70dd0-66ba-4e5e-9ead-001f0ee45516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(cased_lines, tokenizer, basic_tokenizer):\n",
    "    def write_file(fout, cased_lines, mode):\n",
    "        print(f'---------------------{mode} data set')\n",
    "        for i, cased_line in enumerate(tqdm(cased_lines)):\n",
    "            tokens = basic_tokenizer.tokenize(cased_line)\n",
    "            split_tokens = []\n",
    "            for token in tokens:\n",
    "                subtokens = tokenizer.tokenize(token)\n",
    "                split_tokens += subtokens\n",
    "            fout.write(' '.join(split_tokens) + '\\n')\n",
    "    \n",
    "    write_file(open('../data/train_tokened_corpus.txt', 'w'), cased_lines[:(l//10)*8], 'train')\n",
    "    write_file(open('../data/valid_tokened_corpus.txt', 'w'), cased_lines[(l//10)*8:(l//10)*9], 'valid')\n",
    "    write_file(open('../data/test_tokened_corpus.txt', 'w'), cased_lines[(l//10)*9:], 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2752c31-1e63-4de9-93cf-2a6d30ef09cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------train data set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52434944/52434944 [13:12:47<00:00, 1102.33it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------valid data set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6554368/6554368 [1:39:03<00:00, 1102.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------test data set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6554376/6554376 [1:38:31<00:00, 1108.67it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenize(cased_lines, tokenizer, basic_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8260fdf7-82e8-4eae-9acf-ac0b02f8b635",
   "metadata": {},
   "source": [
    "### Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6bd4e74-a86a-4207-802f-b379c49f3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import ray\n",
    "num_cpus = 10\n",
    "ray.init(num_cpus=num_cpus, ignore_reinit_error=True, dashboard_host=\"0.0.0.0\", dashboard_port=8265, include_dashboard=True)\n",
    "\n",
    "def get_chunks(fpath, chunk_num):\n",
    "    with open(fpath) as f:\n",
    "        cased_lines = f.read().splitlines()\n",
    "    random.shuffle(cased_lines)\n",
    "    chunk_size = len(cased_lines) // chunk_num\n",
    "    start = chunk_size\n",
    "    for i in range(chunk_num):\n",
    "        yield cased_lines[start:]\n",
    "        start += chunk_size\n",
    "\n",
    "@ray.remote\n",
    "def tokenize(cased_lines, tokenizer, basic_tokenizer):\n",
    "    sents = []\n",
    "    for cased_line in cased_lines:\n",
    "        tokens = basic_tokenizer.tokenize(cased_line)\n",
    "        split_tokens = []\n",
    "        for token in tokens:\n",
    "            subtokens = tokenizer.tokenize(token)\n",
    "            split_tokens += subtokens\n",
    "        sents.append(split_tokens)\n",
    "    return sents\n",
    "\n",
    "\n",
    "def process(cased_file, output_file, bert_model_type='bert-base-cased', workers=num_cpus):\n",
    "    tokenizer = KoNLPyBertTokenizer(\n",
    "        konlpy_wordpiece = KoNLPyWordPieceTokenizer(Mecab(), use_tag=True),\n",
    "        vocab_file = '../tokenizer/konlpy_tokenizer/vocab.txt'\n",
    "    )\n",
    "    basic_tokenizer = BasicTokenizer(do_lower_case=False)\n",
    "    fout = open(output_file, 'w')\n",
    "    futures = [tokenize.remote(chunked_list) for chunked_list in get_chunks(cased_file)]\n",
    "    results = ray.get(futures)\n",
    "    print(f'total tokenized sentences : {len(results)}')\n",
    "    for lines in tqdm(results):\n",
    "        for i, line in enumerate(lines):\n",
    "            fout.write(' '.join(line) + '\\n')\n",
    "    fout.close()\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08173a0-b122-4c75-b048-62efba81e8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    process('../data/preprocessed_corpus.txt', '../data/tokenized_corpus.txt')\n",
    "except:\n",
    "    ray.shutdown()\n",
    "finally:\n",
    "    ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
