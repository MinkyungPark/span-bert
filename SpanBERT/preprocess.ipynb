{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa683b2-28e5-44fa-aa70-fcc916f04b9c",
   "metadata": {},
   "source": [
    "for fairseq preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8730741f-6e0e-4028-a609-cf299432d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import zip_longest\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "from fairseq.data import indexed_dataset\n",
    "from fairseq.tasks.span_bert import BertDictionary\n",
    "from fairseq.tokenizer import Tokenizer, tokenize_line\n",
    "from multiprocessing import Pool, Manager, Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2035a7-283e-4668-ada7-514af7026f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    print(args)\n",
    "    os.makedirs(args.destdir, exist_ok=True)\n",
    "    target = not args.only_source\n",
    "\n",
    "    def build_dictionary(filenames):\n",
    "        d = BertDictionary()\n",
    "        for filename in filenames:\n",
    "            Tokenizer.add_file_to_dictionary(filename, d, tokenize_line, args.workers)\n",
    "        return d\n",
    "\n",
    "    def train_path(lang):\n",
    "        return '{}{}'.format(args.trainpref, ('.' + lang) if lang else '')\n",
    "\n",
    "    def file_name(prefix, lang):\n",
    "        fname = prefix\n",
    "        if lang is not None:\n",
    "            fname += f'.{lang}'\n",
    "        return fname\n",
    "\n",
    "    def dest_path(prefix, lang):\n",
    "        return os.path.join(args.destdir, file_name(prefix, lang))\n",
    "\n",
    "    def dict_path(lang):\n",
    "        return dest_path('dict', lang) + '.txt'\n",
    "\n",
    "    if args.joined_dictionary:\n",
    "        assert not args.srcdict, 'cannot combine --srcdict and --joined-dictionary'\n",
    "        assert not args.tgtdict, 'cannot combine --tgtdict and --joined-dictionary'\n",
    "        src_dict = build_dictionary(set([\n",
    "            train_path(lang)\n",
    "            for lang in [args.source_lang, args.target_lang]\n",
    "        ]))\n",
    "        tgt_dict = src_dict\n",
    "    else:\n",
    "        if args.srcdict:\n",
    "            src_dict = BertDictionary.load(args.srcdict)\n",
    "        else:\n",
    "            assert args.trainpref, \"--trainpref must be set if --srcdict is not specified\"\n",
    "            src_dict = build_dictionary([train_path(args.source_lang)])\n",
    "        if target:\n",
    "            if args.tgtdict:\n",
    "                tgt_dict = BertDictionary.load(args.tgtdict)\n",
    "            else:\n",
    "                assert args.trainpref, \"--trainpref must be set if --tgtdict is not specified\"\n",
    "                tgt_dict = build_dictionary([train_path(args.target_lang)])\n",
    "\n",
    "    src_dict.finalize(\n",
    "        threshold=args.thresholdsrc,\n",
    "        nwords=args.nwordssrc,\n",
    "        padding_factor=args.padding_factor,\n",
    "    )\n",
    "    src_dict.save(dict_path(args.source_lang))\n",
    "    if target:\n",
    "        if not args.joined_dictionary:\n",
    "            tgt_dict.finalize(\n",
    "                threshold=args.thresholdtgt,\n",
    "                nwords=args.nwordstgt,\n",
    "                padding_factor=args.padding_factor,\n",
    "            )\n",
    "        tgt_dict.save(dict_path(args.target_lang))\n",
    "\n",
    "    def make_binary_dataset(input_prefix, output_prefix, lang, num_workers):\n",
    "        dict = BertDictionary.load(dict_path(lang))\n",
    "        print('| [{}] Dictionary: {} types'.format(lang, len(dict)))\n",
    "        n_seq_tok = [0, 0]\n",
    "        replaced = Counter()\n",
    "\n",
    "        def merge_result(worker_result):\n",
    "            replaced.update(worker_result['replaced'])\n",
    "            n_seq_tok[0] += worker_result['nseq']\n",
    "            n_seq_tok[1] += worker_result['ntok']\n",
    "\n",
    "        input_file = '{}{}'.format(input_prefix, ('.' + lang) if lang is not None else '')\n",
    "        offsets = Tokenizer.find_offsets(input_file, num_workers)\n",
    "        pool = None\n",
    "        if num_workers > 1:\n",
    "            pool = Pool(processes=num_workers-1)\n",
    "            for worker_id in range(1, num_workers):\n",
    "                prefix = \"{}{}\".format(output_prefix, worker_id)\n",
    "                pool.apply_async(binarize, (args, input_file, dict, prefix, lang,\n",
    "                                            offsets[worker_id],\n",
    "                                            offsets[worker_id + 1]), callback=merge_result)\n",
    "            pool.close()\n",
    "\n",
    "        ds = indexed_dataset.IndexedDatasetBuilder(dataset_dest_file(args, output_prefix, lang, 'bin'))\n",
    "        merge_result(Tokenizer.binarize(input_file, dict, lambda t: ds.add_item(t),\n",
    "                                        offset=0, end=offsets[1]))\n",
    "        if num_workers > 1:\n",
    "            pool.join()\n",
    "            for worker_id in range(1, num_workers):\n",
    "                prefix = \"{}{}\".format(output_prefix, worker_id)\n",
    "                temp_file_path = dataset_dest_prefix(args, prefix, lang)\n",
    "                ds.merge_file_(temp_file_path)\n",
    "                os.remove(indexed_dataset.data_file_path(temp_file_path))\n",
    "                os.remove(indexed_dataset.index_file_path(temp_file_path))\n",
    "\n",
    "\n",
    "        ds.finalize(dataset_dest_file(args, output_prefix, lang, 'idx'))\n",
    "\n",
    "\n",
    "        print('| [{}] {}: {} sents, {} tokens, {:.3}% replaced by {}'.format(\n",
    "            lang, input_file, n_seq_tok[0], n_seq_tok[1],\n",
    "            100 * sum(replaced.values()) / n_seq_tok[1], dict.unk_word))\n",
    "\n",
    "\n",
    "\n",
    "    def make_dataset(input_prefix, output_prefix, lang, num_workers=1):\n",
    "        if args.output_format == 'binary':\n",
    "            make_binary_dataset(input_prefix, output_prefix, lang, num_workers)\n",
    "        elif args.output_format == 'raw':\n",
    "            # Copy original text file to destination folder\n",
    "            output_text_file = dest_path(\n",
    "                output_prefix + '.{}-{}'.format(args.source_lang, args.target_lang),\n",
    "                lang,\n",
    "            )\n",
    "            shutil.copyfile(file_name(input_prefix, lang), output_text_file)\n",
    "\n",
    "    def make_all(lang):\n",
    "        if args.trainpref:\n",
    "            make_dataset(args.trainpref, 'train', lang, num_workers=args.workers)\n",
    "        if args.validpref:\n",
    "            for k, validpref in enumerate(args.validpref.split(',')):\n",
    "                outprefix = 'valid{}'.format(k) if k > 0 else 'valid'\n",
    "                make_dataset(validpref, outprefix, lang)\n",
    "        if args.testpref:\n",
    "            for k, testpref in enumerate(args.testpref.split(',')):\n",
    "                outprefix = 'test{}'.format(k) if k > 0 else 'test'\n",
    "                make_dataset(testpref, outprefix, lang)\n",
    "\n",
    "    make_all(args.source_lang)\n",
    "    if target:\n",
    "        make_all(args.target_lang)\n",
    "\n",
    "    print('| Wrote preprocessed data to {}'.format(args.destdir))\n",
    "\n",
    "    if args.alignfile:\n",
    "        assert args.trainpref, \"--trainpref must be set if --alignfile is specified\"\n",
    "        src_file_name = train_path(args.source_lang)\n",
    "        tgt_file_name = train_path(args.target_lang)\n",
    "        src_dict = dictionary.Dictionary.load(dict_path(args.source_lang))\n",
    "        tgt_dict = dictionary.Dictionary.load(dict_path(args.target_lang))\n",
    "        freq_map = {}\n",
    "        with open(args.alignfile, 'r') as align_file:\n",
    "            with open(src_file_name, 'r') as src_file:\n",
    "                with open(tgt_file_name, 'r') as tgt_file:\n",
    "                    for a, s, t in zip_longest(align_file, src_file, tgt_file):\n",
    "                        si = Tokenizer.tokenize(s, src_dict, add_if_not_exist=False)\n",
    "                        ti = Tokenizer.tokenize(t, tgt_dict, add_if_not_exist=False)\n",
    "                        ai = list(map(lambda x: tuple(x.split('-')), a.split()))\n",
    "                        for sai, tai in ai:\n",
    "                            srcidx = si[int(sai)]\n",
    "                            tgtidx = ti[int(tai)]\n",
    "                            if srcidx != src_dict.unk() and tgtidx != tgt_dict.unk():\n",
    "                                assert srcidx != src_dict.pad()\n",
    "                                assert srcidx != src_dict.eos()\n",
    "                                assert tgtidx != tgt_dict.pad()\n",
    "                                assert tgtidx != tgt_dict.eos()\n",
    "\n",
    "                                if srcidx not in freq_map:\n",
    "                                    freq_map[srcidx] = {}\n",
    "                                if tgtidx not in freq_map[srcidx]:\n",
    "                                    freq_map[srcidx][tgtidx] = 1\n",
    "                                else:\n",
    "                                    freq_map[srcidx][tgtidx] += 1\n",
    "\n",
    "        align_dict = {}\n",
    "        for srcidx in freq_map.keys():\n",
    "            align_dict[srcidx] = max(freq_map[srcidx], key=freq_map[srcidx].get)\n",
    "\n",
    "        with open(os.path.join(args.destdir, 'alignment.{}-{}.txt'.format(\n",
    "                args.source_lang, args.target_lang)), 'w') as f:\n",
    "            for k, v in align_dict.items():\n",
    "                print('{} {}'.format(src_dict[k], tgt_dict[v]), file=f)\n",
    "\n",
    "\n",
    "\n",
    "def binarize(args, filename, dict, output_prefix, lang, offset, end):\n",
    "\n",
    "    ds = indexed_dataset.IndexedDatasetBuilder(dataset_dest_file(args, output_prefix, lang, 'bin'))\n",
    "    def consumer(tensor):\n",
    "        ds.add_item(tensor)\n",
    "\n",
    "    res = Tokenizer.binarize(filename, dict, consumer, offset=offset, end=end)\n",
    "    ds.finalize(dataset_dest_file(args, output_prefix, lang, 'idx'))\n",
    "    return res\n",
    "\n",
    "def dataset_dest_prefix(args, output_prefix, lang):\n",
    "    base = f'{args.destdir}/{output_prefix}'\n",
    "    lang_part = f'.{args.source_lang}-{args.target_lang}.{lang}' if lang is not None else ''\n",
    "    return f'{base}{lang_part}'\n",
    "\n",
    "\n",
    "def dataset_dest_file(args, output_prefix, lang, extension):\n",
    "    base = dataset_dest_prefix(args, output_prefix, lang)\n",
    "    return f'{base}.{extension}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03fd3f03-24da-458b-987c-f9624ec4e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictX(dict):\n",
    "    def __getattr__(self, key):\n",
    "        try:\n",
    "            return self[key]\n",
    "        except KeyError as k:\n",
    "            raise AttributeError(k)\n",
    "\n",
    "    def __setattr__(self, key, value):\n",
    "        self[key] = value\n",
    "\n",
    "    def __delattr__(self, key):\n",
    "        try:\n",
    "            del self[key]\n",
    "        except KeyError as k:\n",
    "            raise AttributeError(k)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<DictX ' + dict.__repr__(self) + '>'\n",
    "    \n",
    "args = DictX({\n",
    "    'source_lang': None,\n",
    "    'target_lang': None,\n",
    "    'trainpref': '../data/train_tokened_corpus.txt',\n",
    "    'validpref': '../data/valid_tokened_corpus.txt',\n",
    "    'testpref': '../data/test_tokened_corpus.txt',\n",
    "    'destdir': './train_data/',\n",
    "    'thresholdtg':0,\n",
    "    'thresholdsrc':0,\n",
    "    'tgtdict': None,\n",
    "    'srcdict': None,\n",
    "    'nwordstgt': -1,\n",
    "    'nwordssrc': -1, # number of source words to retain\n",
    "    'alignfile': None,\n",
    "    'output_format':'binary',\n",
    "    'joined_dictionary': False,\n",
    "    'only_source': True,\n",
    "    'padding_factor': 1,\n",
    "    'workers': 48,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cfc5104-3a24-4028-a6d8-c606cbeae9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DictX {'source_lang': None, 'target_lang': None, 'trainpref': '../data/train_tokened_corpus.txt', 'validpref': '../data/valid_tokened_corpus.txt', 'testpref': '../data/test_tokened_corpus.txt', 'destdir': './train_data/', 'thresholdtg': 0, 'thresholdsrc': 0, 'tgtdict': None, 'srcdict': None, 'nwordstgt': -1, 'nwordssrc': -1, 'alignfile': None, 'output_format': 'binary', 'joined_dictionary': False, 'only_source': True, 'padding_factor': 1, 'workers': 48}>\n",
      "| [None] Dictionary: 155769 types\n",
      "| [None] ../data/train_tokened_corpus.txt: 52434944 sents, 2428193651 tokens, 0.0% replaced by [UNK]\n",
      "| [None] Dictionary: 155769 types\n",
      "| [None] ../data/valid_tokened_corpus.txt: 6554368 sents, 303234244 tokens, 4.12e-05% replaced by [UNK]\n",
      "| [None] Dictionary: 155769 types\n",
      "| [None] ../data/test_tokened_corpus.txt: 6554376 sents, 303655889 tokens, 0.000104% replaced by [UNK]\n",
      "| Wrote preprocessed data to ./train_data/\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
