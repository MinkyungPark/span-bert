{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4d71acb-51dd-4df9-a272-f0e6586297fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, WeightedRandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad412bf-dbc4-41a8-83db-ea933834bb3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "음식      91685\n",
       "카페      27855\n",
       "명소      15606\n",
       "술집       8286\n",
       "숙박       7354\n",
       "문화예술     3297\n",
       "시장        993\n",
       "공원        560\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test data set\n",
    "with open('./test_dataset_each_10_500.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ffd2a87-f758-4aa4-a934-3d0ce6712b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: '음식', 7: '시장', 4: '술집', 5: '숙박', 2: '공원', 1: '카페', 6: '문화예술', 0: '명소'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = list(set([str(label) + '$' + category for (label, category, ) in df[['label', 'category']].values.tolist()]))\n",
    "mapping = {int(cate_label.split('$')[0]):cate_label.split('$')[1] for cate_label in tmp}\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd692b78-2f92-4ba2-8d5f-edc94194c8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19553"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = []\n",
    "for i in range(8):\n",
    "    f_df = df[df['label'] == i][['sentence', 'label']].values.tolist()\n",
    "    if len(f_df) >= 3000:\n",
    "        samples = random.sample(f_df, 3000)\n",
    "    else:\n",
    "        samples = random.sample(f_df, len(f_df))\n",
    "    raw_dataset.extend(samples)\n",
    "random.shuffle(raw_dataset)\n",
    "len(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ccf4b6-46af-47fd-ae48-9bae19d49ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['[CLS] ' + t[0] + '[SEP]' for t in raw_dataset]\n",
    "labels = [t[1] for t in raw_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "682ea6b9-ccc3-48e4-bfda-130a41481ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 김해 진영은 갈비가 유명해요 할매갈비,시민갈비,신라가든, 요즘엔 수제갈비도 맛있죠? 전 시민갈비다녀왓는데 맛나게먹고왔었어용 돼지양념갈비로 주문했어요담엔 생갈비도 한번먹어봐야겠어용 기본상에 양념게장이나와서맛나게 먹구요 계란찜도 나왔어요 아기들데리고가기도 좋을것같아요 고기3인분이였던것같아요 갈비가 진짜 크더라구요 왕갈비가 생각났어요 맛있게 구워서 먹기만하면되죵! 돼지갈비는 양념때문에 잘타지않게 자주 뒤집어주는게 팁이예요 맛있게 다먹고된장찌개까지~ 갈비양념이 너무단걸 싫어하는데 여긴적당했던것 같아요 고기가 무척이나먹고싶네요 고기중독 김해맛집 김해고기맛집 진영시민갈비[SEP]\n",
      "['[CLS]', '김해', '진영', '##은', '갈비', '##가', '유명', '##해요', '할매', '##갈비', ',', '시민', '##갈비', ',', '신라', '##가', '##든', ',', '요즘', '##엔', '수제', '##갈비', '##도', '맛있', '##죠', '?', '전', '시민', '##갈비', '##다녀', '##왓', '##는데', '맛나', '##게', '##먹', '##고', '##왔', '##었', '##어', '##용', '돼지', '##양', '##념', '##갈비', '##로', '주문', '##했', '##어요', '##담', '##엔', '생', '##갈비', '##도', '한번', '##먹', '##어', '##봐야', '##겠', '##어', '##용', '기본', '##상', '##에', '양념', '##게', '##장', '##이나', '##와서', '##맛', '##나', '##게', '먹', '##구요', '계란찜', '##도', '나왔', '##어요', '아기', '##들', '##데리', '##고', '##가', '##기', '##도', '좋', '##을', '##것', '##같', '##아요', '고기', '##3', '##인', '##분', '##이', '##였', '##던', '##것', '##같', '##아요', '갈비', '##가', '진짜', '크', '##더라구요', '왕', '##갈비', '##가', '생각났', '##어요', '맛있', '##게', '구워서', '먹', '##기', '##만', '##하면', '##되', '##죵', '!', '돼지갈비', '##는', '양념', '##때', '##문', '##에', '잘', '##타', '##지', '##않', '##게', '자주', '뒤집', '##어주', '##는', '##게', '팁', '##이', '##예', '##요', '맛있', '##게', '다', '##먹', '##고', '##된장', '##찌개', '##까', '##지', '~', '갈비', '##양', '##념', '##이', '너무', '##단', '##걸', '싫어하', '##는데', '여긴', '##적', '##당', '##했', '##던', '##것', '같', '##아요', '고기', '##가', '무척', '##이나', '##먹', '##고', '##싶', '##네요', '고기', '##중', '##독', '김해', '##맛집', '김해', '##고기', '##맛집', '진영', '##시', '##민', '##갈비', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "user_defined_symbols = ['[PAD]', '[UNK]', '[UNK0]','[UNK1]','[UNK2]','[UNK3]','[UNK4]','[UNK5]','[UNK6]','[UNK7]','[UNK8]','[UNK9]', '[CLS]', '[SEP]', '[MASK]', '[BOS]','[EOS]']\n",
    "unused_token_num = 200\n",
    "unused_list = ['[unused{}]'.format(n) for n in range(unused_token_num)]\n",
    "user_defined_symbols = user_defined_symbols + unused_list\n",
    "\n",
    "tokenizer = BertTokenizer(\n",
    "    vocab_file = './hf_tokenizer_special/vocab.txt',\n",
    "    max_len = 1502,\n",
    "    do_lower_case=False,\n",
    ")\n",
    "special_token_dic = {'additional_special_tokens': user_defined_symbols}\n",
    "tokenizer.add_special_tokens(special_token_dic)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "\n",
    "print(sentences[0])\n",
    "print(tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b100587-cb2a-4a25-ae43-a2250b5c2579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    2,  3997,  4235,     6,   537,  4705,  2709, 10123,  4668,\n",
       "        4536,  4240,  2981,  4505,  4391,   906, 23287,  4534,  4646,\n",
       "        7488,  4252,  6472,  7461,  4444,  2769, 11694,  2755,  4214,\n",
       "       10123,   176,  4579, 14418,  8021,  4196,  4235, 10123, 13117,\n",
       "        4134,  4465,  4240,  9937,  4290,  4045,  4235,  5174,  4214,\n",
       "        1230,  5120,  4240,  6761,  4134,  4465,  7486, 18246,     9,\n",
       "       10429,  4362,    17,  4772,  7461,  3251,  4668,  9899, 10392,\n",
       "       23193, 10911, 20838,  9055,  4444,  7356,  4214,  6666,  7019,\n",
       "        4274, 12709,  4178,  4362,  2828,  4225,  4616, 29099,  4377,\n",
       "        4477,  5086,  8572,  2546,  4626,  4421,  4800,  4478,  8743,\n",
       "        9899, 14644,  4240,  1374,  4616,  4423,  2594,  4367, 14948,\n",
       "        4494,  2141,  4705, 26004,  4666,  4240, 26896,  2097,  4444,\n",
       "        9717, 12901,  4299,     7, 25304,  4236,     7,  2690, 10457,\n",
       "        4240,  2741,  4209,     7, 22280,     7,  7201, 20778,  4367,\n",
       "        2738,  4633, 12538,  4260,  4214,   205,  4190,     7,  6843,\n",
       "           7, 16586,  4633,  7488,  4401, 26326,  4203, 13246, 26793,\n",
       "        4444, 10973,  4214,  9608,  4494, 11479,  4290,  6586,  4494,\n",
       "        7710,  4486,  4401,  7045,  4633, 11994, 29883,   907,   137,\n",
       "       18246,     9,  9338,    21,  7019,  4354,  4537,     6, 10392,\n",
       "        7488,  3997,  4235,   537,  4705,  4478,  7683,  4260,  4191,\n",
       "        6736, 10392,  7358, 10911,  4444, 13376,     9,  7286, 10392,\n",
       "        7358, 10911,  4274,  6737, 16753,  4399,  4196, 18605,     9,\n",
       "       11862,  4260,  4185,  4214, 19883,  4391,  1396,  4285,  4220,\n",
       "        7488,  3997,  4235,   537,  4705,  4274,  2862,  8532,  6991,\n",
       "        7122,  3919,   137,    96,  5651,  4177,     9,  9246,  7999,\n",
       "        7509,  4220,  8532,  4274,  7075,  7993,  4385,  4223,  4240,\n",
       "        8109,  4505,  4354,  4380, 17264,  2746,  4478,  2136,  2755,\n",
       "        4478,   137,    96,  4177,     9,  7571,  4237,  6660,  4274,\n",
       "        7571,  9899, 13547,  1617,  5651,  7586,  9968,  6619,   519,\n",
       "          75,  6952,  4478,  7827,  4176, 15526,  4499,  4196,  2862,\n",
       "        7978,  4391,  2755,  4214,   137,    96,  4177,     9, 22280,\n",
       "        4401,  4327,  7805,  4196,  2755,  4291,  7691,  4367,  6526,\n",
       "        7683,  4260,  9408,  3944,  9223,  7691,  4196,  7949,  4196,\n",
       "       15364,  5174,  4291,  1181,  4367,  6526,  2564,   137,  4196,\n",
       "       22939,  4177,     9, 19219,  4391,  1396,  4274,  4387,  4237,\n",
       "        7488,  4444,  7157, 11214,  4196,  8544, 10483,     9,  1396,\n",
       "        4274, 11660,  4223,  4237,  9657,    75,  6501, 17470,     7,\n",
       "       16753,     7,  6501, 22280,  4401,  4196,  4472,  9899,   763,\n",
       "        4504,  4214,  3997,  4345,  4341,  4327,  4223,  4444,  4371,\n",
       "        6843,  4367, 21976,  4478,  2564,  5546,  4478,   763,  4330,\n",
       "       12437,  4214,  2814,  4331,  4391,  8381,  4214,   973, 14645,\n",
       "        4262,     9, 11214,  4478,   763,  4494,  7993,  4385,  4223,\n",
       "        4732, 12080,  4633, 28466,     9,  1181,  4367,  4240,  6905,\n",
       "        1394,  9899,  4237,  6841,  4494,   544,  4196, 18605,     9,\n",
       "        7286,  2993,  9899,  9330,  4214,   322,  4444,  7835,  8738,\n",
       "        4196,     9,  9699,  4213,  4235,  3912,  4214, 19883, 26071,\n",
       "       13018,  7488,  7135,  4391,  9580,  9659,  4214,  6523,  4196,\n",
       "        2862,  7566,  6682,     9, 12135,     7, 16839,  4169,  4478,\n",
       "       18183, 17080,  7358,  4169,  4448,  4478, 13471,  4260,  4214,\n",
       "         137,  2871,  4274,   137, 24587,  4213,  4448,  3919,  2136,\n",
       "        2497,  4478,   137,    96, 18304,     9,  7488,  3997,  4235,\n",
       "         537,  4705, 27446,  9699,  4214, 21976,  4240,  7546,  4196,\n",
       "        7488, 12249, 13014,  5174, 26993,  3919,   973,     9,  9699,\n",
       "        4213,  4235,  3912,  4214,  7805,  4478,  9197,     7, 12135,\n",
       "        4290,  6672,  4260,  4214,   137,  4237,  2871,  4274,  7354,\n",
       "          96, 18304,     9, 19219,  4391,  2755,  4214,  1760,  4223,\n",
       "        4448,  4240,  9544,  4391,   904,  4214,   137,  4196])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "# token to index number\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "# padding\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "982e72e8-63c5-4852-908d-783c14f3d996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "attention_masks = []\n",
    "# padding 0, non padding 1\n",
    "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "154448be-8dda-498b-a4d0-933d9c4a8c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2, 18166,  7170,  8933,  4177,  4263, 29881,  7541,  6642,  4260,\n",
      "         4177, 10709,  4616,  4391,  4364, 10859,  6681,  4391,  4454, 27523,\n",
      "         2244,  4478,  1019,  4391,  2755,  6682,  4535,  4478,  6500,  6740,\n",
      "         4304,  7557,  4214,  3807, 24587,  4611,  4744, 17080,  2244,  4478,\n",
      "         1019, 15203,  6476,  4478,  7716,  4305,  3912,  9223,  4262,  4196,\n",
      "        11525,  4214,  9560,  4444,  6552,  4494,  6681,  6476,  4444,  6748,\n",
      "         6682,    12,  4237,  4462,  6681,  8417, 10326,  4268,  4477,  4801,\n",
      "         7424,  7637, 22415,  4780,  4290, 12446,  4329,  4263,  6870,    21,\n",
      "         6532,     8,  7456,    21,  6532,  4233,  4747,  4592,     7,  6538,\n",
      "            7,  6662,     7, 11931,  8863,     7,   547,    10,  2525,  6982,\n",
      "         9316, 14389,  8466,     8, 24133,  4183,     8, 27751,  4180, 23480,\n",
      "           21,    10,    10,  7514,     9,  8052,     9,  6658,    10, 11317,\n",
      "         7377,    12,  4282,  4280,  7508,  4444,  7365,  4236,  4203,  2420,\n",
      "         4300,  4236,  6612,  4633, 18192, 13376,  9930,  3912,  4186,  4220,\n",
      "         4537,  5651,  4230,    12,  4237,  4462,  6681,  8417, 10326,  4268,\n",
      "        29881, 10412,  7482,  6507,  2544,  4444,  6552,  4330,  2755,  6682,\n",
      "           12,  4237,  4462,  6681,  8417, 10326,  4268,  4391,  4371,  7515,\n",
      "         4444,  2755,  4214, 21728,  4316,  4262,   430,  4196, 11367,  3897,\n",
      "         4229, 18605,  6722, 10312,  4478, 10238,  4214,   137,  4274,  5380,\n",
      "        27446,  7921,  6533,  4214,  4578,  4494,  7536,     5,  9091,  7537,\n",
      "        24587,  4176,  4535,  7030,  4214,  6541,  4214,  4620,  4196, 21766,\n",
      "         4304, 12633,  6583,  4663,  4304,  4291,  7171,  4284,  4284,  7537,\n",
      "         4196,  4233,  4532, 19058,  4253,     5,  6490,  7646,  4627,  2871,\n",
      "         4274,  7374, 18605,  4178,  4772,  1398,  4380,  6862,  7303,  2916,\n",
      "        24587,  4213,  3912,  4176,  4225,  4772,  4196, 10214,  4191,  2811,\n",
      "        27446,  5187,  4397,  6537,  4330,  4537,  7170,  7518, 19666, 14838,\n",
      "         4367,  5808,  4213,  4240,  6681,  4391,   904,  4191,  4380,  4304,\n",
      "        12790,  6799,    12,  4237,  4462,  4240, 19643,  7507,  4478,  4314,\n",
      "        18902,  4214,  8784,  4237,  2755,  6682,  6614,  4391,   322, 10556,\n",
      "         7028,  6974, 20432, 22364,  4237,  6573,  4379,  1394,  4330,  8535,\n",
      "         4304,  4599,  4274,  7072,  9508,  4196,  4176,  4212,  4321,  4264,\n",
      "         9899,  6537,  4330,  1710,  5651,  6682,  6942,  6938,  8373,  1396,\n",
      "        12633,  6583,    75,  7241,  4444, 15443,  4453,  4237,  6844,  4220,\n",
      "        14699,  2421,  4285,  4237,  9460,  4391,  2497,  4229,  6682,    12,\n",
      "         4237,  4462,  6681,  4240,  6722,  2871,  4274,  2817,  4274,  4611,\n",
      "         4233, 29883,  6681,  4633,  6740,  4191,  2871,  4274,  8940,  4223,\n",
      "         4237,  2755,  4213,  4193,  4193, 29883,  6580, 13134,  6573,  4494,\n",
      "         8940,  4223,  4237,  2755, 10556,  4208,  4667,  4444,  6735,  6820,\n",
      "         4379,  2136,  2755,  6682,  6543,  4237,  6718,  6856,  4494,  3807,\n",
      "        24587,  4512,  4358,  6697,  6871,  4791,  2136,  2755,  6682,  6544,\n",
      "         4214,  6580,  4633,  8093,  2564,  4068,  4174,  4611,  4233,  4494,\n",
      "         6929,  4633,  6820,  4602,  6682,  6559,  8080,  4444,  4214,    12,\n",
      "         4237,  4462,  6681, 27446,  6530,  4260,  4214,  6481,  4223,  4448,\n",
      "         9440,  4820,  5651,  6682,  6481,  4391,  6479,  1396,  4285,  7167,\n",
      "         6929,  4633, 18422, 14137,  6716, 11258,  4186,  4485,  4514,  6481,\n",
      "         2916,  7549,  4186,   904,  5225,  4304,  1548,  4316,  4262,  6681,\n",
      "         7537,  7374,  4895,  4448,  6516,  4174,  4334,  9088,  7374,  4237,\n",
      "         7303,  2916,  4196, 18605,  4184,  4237,  4462,  6681,  6614,  6495,\n",
      "         4367,  6481,  6495,  4478,  9210,  4444,  4355,  4459,  4068,  6875,\n",
      "         4242,  4444,  4371,  6853, 25053,  4678, 15203,  7171,  4284,  4284,\n",
      "         3915,  2766,  4478,  4286,  4348,  6664,  3912,  4176,  9210,  4391,\n",
      "         2755,  4214])\n",
      "tensor(4)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "tensor([    2,  6727,  4444,  9787, 10392,  4444,  7488,  4478, 25998,    91,\n",
      "         6682,    75,  6507, 14044,   537,  4240, 21640, 16457,     5,  6610,\n",
      "        10392, 27446,  6778,  3927,  4260,  7324,  7488,  8334, 10975, 27764,\n",
      "         6694,  7967, 10392,  4226,  4391,  4682,  6599,    14,  4499,  9787,\n",
      "         4444,  9707,    12,  4271,  4767,  4216,  4290,  7044,  4220,    14,\n",
      "         4242,  6524, 10577, 10975, 27764,  4196,  7506,     5,  2636,  5047,\n",
      "         4371,  9066,  4196,  4203, 19991,  4511,  4274,  6504,  4327,  4391,\n",
      "           22,  3912,  4186,  4220,  7827,  4177,  3732,  4584,  4870,  6799,\n",
      "         1003,  9325,  6732,  4602, 18605,  6937,  4245,  7488,  6662,  4602,\n",
      "         9223,  2583,  6507, 10303,  4235,  4454,  4235, 27936,  6558,  2908,\n",
      "         2049,  6469,  7143,  4391,  6490, 12745,  4277, 18605,     5,   537,\n",
      "         4240, 21640,  4538,  4189,  4685,  6812,  6442,  6469,  6626,  6498,\n",
      "         4380,  4304,  6560, 29361,  4220, 16129,  6788,  4444, 11495,  6983,\n",
      "         5132,  6861,     5,  6507,  2544,  4444,  2755, 10556,  9217,  7569,\n",
      "           75,  6944,  4196,  6500,  1764,  4315,  3944,  6862,  6655,  7358,\n",
      "         4478,  7511,  6513,  4223,  4240, 11214,  9899, 14636, 10392, 25619,\n",
      "         6915,  4602,  6682,    75,  6983,  2862,  3912,  9234,  4176,  6643,\n",
      "         7205,  4379,  6498,  8334,   537,  4240, 21640,  4538,  4189,  4685,\n",
      "         2738,  4758,  4507,     7,  2560,  4328,  4746,     7, 18257,  4906,\n",
      "            7,  2738,  4518,  4285, 19920,  6520,  4240,  7993,  4223,  6592,\n",
      "         7993,  4214,  7993, 18605,     5,  8614,  4213,  6849,  4213,  6812,\n",
      "         4448,  4841,  6490,  6513,  8373,  6500, 25998,  2593,  9223,  9055,\n",
      "         4391,  6654,  3186, 16967,     5,  6548,  4214,   837,  2738,  4758,\n",
      "         4906,  7993,  4391, 10591,  4668,  4286,  4260,  4547,  8303,  4633,\n",
      "         3935,  4286, 13410,  8922,  8272,  7763,  8272,  9316,  4196,  2420,\n",
      "         5174,  4214,  1461, 18488, 10591,  4791,  4419,   537,  4240, 21640,\n",
      "         4538,  4189,  4685,  6768,  4274, 17129,  4494,  4387,  4345,  4791,\n",
      "         7805,  4237,  6716,  4213,    91,  6682,     5,  7286, 14306,  7993,\n",
      "         4343,  4444,  8395,  4668,  4191,  4419,  4444,  8395,  6578,  4444,\n",
      "         8843,  4505,  7229,  4260,  4214,  1760,  4196,  7993,  4263,  4314,\n",
      "         4174,  4214,  6634,  4274,   466, 12249,  7469, 16967, 14988,  6519,\n",
      "         2384,  4213,  1670,  4213,  7501, 23111,  6562,  4391,  4029, 12602,\n",
      "         4186,  4220,  3540,     9,  6778, 18280, 27446,  1735,  9223,  7993,\n",
      "         4223,  4240,  9758,  7907,  7907, 12745,  4277,  4371,  1715,  2136,\n",
      "         2755, 10556,  6479,  2871,  5651,  6682,     5,   767,  1482,  4288,\n",
      "         4448,  4444,  6562,  4391,  8354,  9223, 11145,  4478, 10208,  2774,\n",
      "         4214, 27090,  4444,  6503,  8299, 16777, 15147,  4274, 19920,  4327,\n",
      "         7993,  4242,  8373,  8322,  4196,  8344,  3912,  4214, 13288,    22,\n",
      "            5,  6503,  2821,  4242,  4652,  4314,    96,  4274,  4387,     5,\n",
      "         8324,  8922, 12601,     5,  9523,  4223,  4240,  6843,  4367, 21291,\n",
      "        27446,  6615, 13209,   537,  4240, 21640,  4538,  4189,  4685,  4668,\n",
      "         4327,     7,  7724,     7,  6541,  7854,  4425, 16068,  9217,  1710,\n",
      "         4191,  2871, 16967,     3,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n",
      "tensor(6)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# seperate train set, val set\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
    "                                                                                    labels, \n",
    "                                                                                    random_state=2018, \n",
    "                                                                                    test_size=0.1)\n",
    "\n",
    "# seperate attention mask with train set, val set\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
    "                                                       input_ids,\n",
    "                                                       random_state=2018, \n",
    "                                                       test_size=0.1)\n",
    "\n",
    "# data to torch tensor\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "print(train_inputs[0])\n",
    "print(train_labels[0])\n",
    "print(train_masks[0])\n",
    "print(validation_inputs[0])\n",
    "print(validation_labels[0])\n",
    "print(validation_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e0743e2-9f85-4853-b49c-4008eb46126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = WeightedRandomSampler(train_data.weights, train_data.data_size) # RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "920e6b55-2d81-4387-854b-e5843caedb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# check gpu\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('No GPU available, using the CPU instead.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b754e2ea-efed-4112-bf38-be0b026665bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at model_output were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at model_output and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model load\n",
    "pretrained_model_config = BertConfig.from_pretrained('model_output')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'model_output',\n",
    "    num_labels=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86c43e79-4a76-4475-baa4-e32ee24c1738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a813dc7b-18d3-4743-8937-9e7479784b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # learning rate\n",
    "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
    "                )\n",
    "epochs = 5\n",
    "total_steps = len(train_dataloader) * epochs # 배치반복 횟수 * 에폭\n",
    "\n",
    "# learning rate 조정을 위한 scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2b895ae-6e34-4be1-a202-706735a736fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of calculation accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b3fb25c-0e9e-4fb0-b6ad-3404ff0f965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function of time check\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded)) # hh:mm:ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f29479c-5c69-4253-951c-4b391a6efb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n",
      "  Batch   500  of  17,597.    Elapsed: 0:00:27.\n",
      "  Batch 1,000  of  17,597.    Elapsed: 0:00:53.\n",
      "  Batch 1,500  of  17,597.    Elapsed: 0:01:20.\n",
      "  Batch 2,000  of  17,597.    Elapsed: 0:01:46.\n",
      "  Batch 2,500  of  17,597.    Elapsed: 0:02:12.\n",
      "  Batch 3,000  of  17,597.    Elapsed: 0:02:38.\n",
      "  Batch 3,500  of  17,597.    Elapsed: 0:03:05.\n",
      "  Batch 4,000  of  17,597.    Elapsed: 0:03:31.\n",
      "  Batch 4,500  of  17,597.    Elapsed: 0:03:56.\n",
      "  Batch 5,000  of  17,597.    Elapsed: 0:04:22.\n",
      "  Batch 5,500  of  17,597.    Elapsed: 0:04:48.\n",
      "  Batch 6,000  of  17,597.    Elapsed: 0:05:14.\n",
      "  Batch 6,500  of  17,597.    Elapsed: 0:05:40.\n",
      "  Batch 7,000  of  17,597.    Elapsed: 0:06:05.\n",
      "  Batch 7,500  of  17,597.    Elapsed: 0:06:31.\n",
      "  Batch 8,000  of  17,597.    Elapsed: 0:06:57.\n",
      "  Batch 8,500  of  17,597.    Elapsed: 0:07:23.\n",
      "  Batch 9,000  of  17,597.    Elapsed: 0:07:49.\n",
      "  Batch 9,500  of  17,597.    Elapsed: 0:08:14.\n",
      "  Batch 10,000  of  17,597.    Elapsed: 0:08:40.\n",
      "  Batch 10,500  of  17,597.    Elapsed: 0:09:06.\n",
      "  Batch 11,000  of  17,597.    Elapsed: 0:09:32.\n",
      "  Batch 11,500  of  17,597.    Elapsed: 0:09:58.\n",
      "  Batch 12,000  of  17,597.    Elapsed: 0:10:24.\n",
      "  Batch 12,500  of  17,597.    Elapsed: 0:10:50.\n",
      "  Batch 13,000  of  17,597.    Elapsed: 0:11:15.\n",
      "  Batch 13,500  of  17,597.    Elapsed: 0:11:41.\n",
      "  Batch 14,000  of  17,597.    Elapsed: 0:12:07.\n",
      "  Batch 14,500  of  17,597.    Elapsed: 0:12:33.\n",
      "  Batch 15,000  of  17,597.    Elapsed: 0:12:58.\n",
      "  Batch 15,500  of  17,597.    Elapsed: 0:13:24.\n",
      "  Batch 16,000  of  17,597.    Elapsed: 0:13:50.\n",
      "  Batch 16,500  of  17,597.    Elapsed: 0:14:16.\n",
      "  Batch 17,000  of  17,597.    Elapsed: 0:14:42.\n",
      "  Batch 17,500  of  17,597.    Elapsed: 0:15:08.\n",
      "\n",
      "  Average training loss: 0.72\n",
      "  Training epcoh took: 0:15:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.90\n",
      "  Validation took: 0:00:22\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "Training...\n",
      "  Batch   500  of  17,597.    Elapsed: 0:00:26.\n",
      "  Batch 1,000  of  17,597.    Elapsed: 0:00:52.\n",
      "  Batch 1,500  of  17,597.    Elapsed: 0:01:17.\n",
      "  Batch 2,000  of  17,597.    Elapsed: 0:01:43.\n",
      "  Batch 2,500  of  17,597.    Elapsed: 0:02:10.\n",
      "  Batch 3,000  of  17,597.    Elapsed: 0:02:36.\n",
      "  Batch 3,500  of  17,597.    Elapsed: 0:03:01.\n",
      "  Batch 4,000  of  17,597.    Elapsed: 0:03:27.\n",
      "  Batch 4,500  of  17,597.    Elapsed: 0:03:53.\n",
      "  Batch 5,000  of  17,597.    Elapsed: 0:04:19.\n",
      "  Batch 5,500  of  17,597.    Elapsed: 0:04:45.\n",
      "  Batch 6,000  of  17,597.    Elapsed: 0:05:11.\n",
      "  Batch 6,500  of  17,597.    Elapsed: 0:05:37.\n",
      "  Batch 7,000  of  17,597.    Elapsed: 0:06:03.\n",
      "  Batch 7,500  of  17,597.    Elapsed: 0:06:29.\n",
      "  Batch 8,000  of  17,597.    Elapsed: 0:06:55.\n",
      "  Batch 8,500  of  17,597.    Elapsed: 0:07:21.\n",
      "  Batch 9,000  of  17,597.    Elapsed: 0:07:47.\n",
      "  Batch 9,500  of  17,597.    Elapsed: 0:08:13.\n",
      "  Batch 10,000  of  17,597.    Elapsed: 0:08:39.\n",
      "  Batch 10,500  of  17,597.    Elapsed: 0:09:05.\n",
      "  Batch 11,000  of  17,597.    Elapsed: 0:09:30.\n",
      "  Batch 11,500  of  17,597.    Elapsed: 0:09:57.\n",
      "  Batch 12,000  of  17,597.    Elapsed: 0:10:23.\n",
      "  Batch 12,500  of  17,597.    Elapsed: 0:10:48.\n",
      "  Batch 13,000  of  17,597.    Elapsed: 0:11:14.\n",
      "  Batch 13,500  of  17,597.    Elapsed: 0:11:41.\n",
      "  Batch 14,000  of  17,597.    Elapsed: 0:12:07.\n",
      "  Batch 14,500  of  17,597.    Elapsed: 0:12:33.\n",
      "  Batch 15,000  of  17,597.    Elapsed: 0:12:59.\n",
      "  Batch 15,500  of  17,597.    Elapsed: 0:13:25.\n",
      "  Batch 16,000  of  17,597.    Elapsed: 0:13:51.\n",
      "  Batch 16,500  of  17,597.    Elapsed: 0:14:16.\n",
      "  Batch 17,000  of  17,597.    Elapsed: 0:14:42.\n",
      "  Batch 17,500  of  17,597.    Elapsed: 0:15:08.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epcoh took: 0:15:13\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation took: 0:00:22\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "Training...\n",
      "  Batch   500  of  17,597.    Elapsed: 0:00:26.\n",
      "  Batch 1,000  of  17,597.    Elapsed: 0:00:52.\n",
      "  Batch 1,500  of  17,597.    Elapsed: 0:01:18.\n",
      "  Batch 2,000  of  17,597.    Elapsed: 0:01:44.\n",
      "  Batch 2,500  of  17,597.    Elapsed: 0:02:09.\n",
      "  Batch 3,000  of  17,597.    Elapsed: 0:02:35.\n",
      "  Batch 3,500  of  17,597.    Elapsed: 0:03:00.\n",
      "  Batch 4,000  of  17,597.    Elapsed: 0:03:26.\n",
      "  Batch 4,500  of  17,597.    Elapsed: 0:03:52.\n",
      "  Batch 5,000  of  17,597.    Elapsed: 0:04:17.\n",
      "  Batch 5,500  of  17,597.    Elapsed: 0:04:43.\n",
      "  Batch 6,000  of  17,597.    Elapsed: 0:05:09.\n",
      "  Batch 6,500  of  17,597.    Elapsed: 0:05:35.\n",
      "  Batch 7,000  of  17,597.    Elapsed: 0:06:01.\n",
      "  Batch 7,500  of  17,597.    Elapsed: 0:06:27.\n",
      "  Batch 8,000  of  17,597.    Elapsed: 0:06:53.\n",
      "  Batch 8,500  of  17,597.    Elapsed: 0:07:19.\n",
      "  Batch 9,000  of  17,597.    Elapsed: 0:07:45.\n",
      "  Batch 9,500  of  17,597.    Elapsed: 0:08:11.\n",
      "  Batch 10,000  of  17,597.    Elapsed: 0:08:37.\n",
      "  Batch 10,500  of  17,597.    Elapsed: 0:09:03.\n",
      "  Batch 11,000  of  17,597.    Elapsed: 0:09:29.\n",
      "  Batch 11,500  of  17,597.    Elapsed: 0:09:55.\n",
      "  Batch 12,000  of  17,597.    Elapsed: 0:10:21.\n",
      "  Batch 12,500  of  17,597.    Elapsed: 0:10:47.\n",
      "  Batch 13,000  of  17,597.    Elapsed: 0:11:12.\n",
      "  Batch 13,500  of  17,597.    Elapsed: 0:11:38.\n",
      "  Batch 14,000  of  17,597.    Elapsed: 0:12:04.\n",
      "  Batch 14,500  of  17,597.    Elapsed: 0:12:29.\n",
      "  Batch 15,000  of  17,597.    Elapsed: 0:12:55.\n",
      "  Batch 15,500  of  17,597.    Elapsed: 0:13:21.\n",
      "  Batch 16,000  of  17,597.    Elapsed: 0:13:47.\n",
      "  Batch 16,500  of  17,597.    Elapsed: 0:14:12.\n",
      "  Batch 17,000  of  17,597.    Elapsed: 0:14:38.\n",
      "  Batch 17,500  of  17,597.    Elapsed: 0:15:04.\n",
      "\n",
      "  Average training loss: 0.20\n",
      "  Training epcoh took: 0:15:09\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation took: 0:00:22\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "Training...\n",
      "  Batch   500  of  17,597.    Elapsed: 0:00:26.\n",
      "  Batch 1,000  of  17,597.    Elapsed: 0:00:51.\n",
      "  Batch 1,500  of  17,597.    Elapsed: 0:01:17.\n",
      "  Batch 2,000  of  17,597.    Elapsed: 0:01:43.\n",
      "  Batch 2,500  of  17,597.    Elapsed: 0:02:09.\n",
      "  Batch 3,000  of  17,597.    Elapsed: 0:02:35.\n",
      "  Batch 3,500  of  17,597.    Elapsed: 0:03:00.\n",
      "  Batch 4,000  of  17,597.    Elapsed: 0:03:26.\n",
      "  Batch 4,500  of  17,597.    Elapsed: 0:03:52.\n",
      "  Batch 5,000  of  17,597.    Elapsed: 0:04:17.\n",
      "  Batch 5,500  of  17,597.    Elapsed: 0:04:43.\n",
      "  Batch 6,000  of  17,597.    Elapsed: 0:05:09.\n",
      "  Batch 6,500  of  17,597.    Elapsed: 0:05:35.\n",
      "  Batch 7,000  of  17,597.    Elapsed: 0:06:00.\n",
      "  Batch 7,500  of  17,597.    Elapsed: 0:06:26.\n",
      "  Batch 8,000  of  17,597.    Elapsed: 0:06:51.\n",
      "  Batch 8,500  of  17,597.    Elapsed: 0:07:17.\n",
      "  Batch 9,000  of  17,597.    Elapsed: 0:07:43.\n",
      "  Batch 9,500  of  17,597.    Elapsed: 0:08:09.\n",
      "  Batch 10,000  of  17,597.    Elapsed: 0:08:34.\n",
      "  Batch 10,500  of  17,597.    Elapsed: 0:09:00.\n",
      "  Batch 11,000  of  17,597.    Elapsed: 0:09:25.\n",
      "  Batch 11,500  of  17,597.    Elapsed: 0:09:51.\n",
      "  Batch 12,000  of  17,597.    Elapsed: 0:10:17.\n",
      "  Batch 12,500  of  17,597.    Elapsed: 0:10:43.\n",
      "  Batch 13,000  of  17,597.    Elapsed: 0:11:08.\n",
      "  Batch 13,500  of  17,597.    Elapsed: 0:11:34.\n",
      "  Batch 14,000  of  17,597.    Elapsed: 0:12:00.\n",
      "  Batch 14,500  of  17,597.    Elapsed: 0:12:25.\n",
      "  Batch 15,000  of  17,597.    Elapsed: 0:12:51.\n",
      "  Batch 15,500  of  17,597.    Elapsed: 0:13:17.\n",
      "  Batch 16,000  of  17,597.    Elapsed: 0:13:43.\n",
      "  Batch 16,500  of  17,597.    Elapsed: 0:14:09.\n",
      "  Batch 17,000  of  17,597.    Elapsed: 0:14:35.\n",
      "  Batch 17,500  of  17,597.    Elapsed: 0:15:00.\n",
      "\n",
      "  Average training loss: 0.10\n",
      "  Training epcoh took: 0:15:05\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.92\n",
      "  Validation took: 0:00:22\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "Training...\n",
      "  Batch   500  of  17,597.    Elapsed: 0:00:26.\n",
      "  Batch 1,000  of  17,597.    Elapsed: 0:00:51.\n",
      "  Batch 1,500  of  17,597.    Elapsed: 0:01:17.\n",
      "  Batch 2,000  of  17,597.    Elapsed: 0:01:43.\n",
      "  Batch 2,500  of  17,597.    Elapsed: 0:02:09.\n",
      "  Batch 3,000  of  17,597.    Elapsed: 0:02:35.\n",
      "  Batch 3,500  of  17,597.    Elapsed: 0:03:00.\n",
      "  Batch 4,000  of  17,597.    Elapsed: 0:03:26.\n",
      "  Batch 4,500  of  17,597.    Elapsed: 0:03:52.\n",
      "  Batch 5,000  of  17,597.    Elapsed: 0:04:18.\n",
      "  Batch 5,500  of  17,597.    Elapsed: 0:04:43.\n",
      "  Batch 6,000  of  17,597.    Elapsed: 0:05:09.\n",
      "  Batch 6,500  of  17,597.    Elapsed: 0:05:35.\n",
      "  Batch 7,000  of  17,597.    Elapsed: 0:06:01.\n",
      "  Batch 7,500  of  17,597.    Elapsed: 0:06:27.\n",
      "  Batch 8,000  of  17,597.    Elapsed: 0:06:53.\n",
      "  Batch 8,500  of  17,597.    Elapsed: 0:07:19.\n",
      "  Batch 9,000  of  17,597.    Elapsed: 0:07:44.\n",
      "  Batch 9,500  of  17,597.    Elapsed: 0:08:10.\n",
      "  Batch 10,000  of  17,597.    Elapsed: 0:08:36.\n",
      "  Batch 10,500  of  17,597.    Elapsed: 0:09:01.\n",
      "  Batch 11,000  of  17,597.    Elapsed: 0:09:27.\n",
      "  Batch 11,500  of  17,597.    Elapsed: 0:09:53.\n",
      "  Batch 12,000  of  17,597.    Elapsed: 0:10:18.\n",
      "  Batch 12,500  of  17,597.    Elapsed: 0:10:44.\n",
      "  Batch 13,000  of  17,597.    Elapsed: 0:11:09.\n",
      "  Batch 13,500  of  17,597.    Elapsed: 0:11:35.\n",
      "  Batch 14,000  of  17,597.    Elapsed: 0:12:01.\n",
      "  Batch 14,500  of  17,597.    Elapsed: 0:12:26.\n",
      "  Batch 15,000  of  17,597.    Elapsed: 0:12:52.\n",
      "  Batch 15,500  of  17,597.    Elapsed: 0:13:18.\n",
      "  Batch 16,000  of  17,597.    Elapsed: 0:13:43.\n",
      "  Batch 16,500  of  17,597.    Elapsed: 0:14:09.\n",
      "  Batch 17,000  of  17,597.    Elapsed: 0:14:34.\n",
      "  Batch 17,500  of  17,597.    Elapsed: 0:15:00.\n",
      "\n",
      "  Average training loss: 0.04\n",
      "  Training epcoh took: 0:15:05\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.93\n",
      "  Validation took: 0:00:22\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# 재현을 위해 랜덤시드 고정\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "model.zero_grad() # initialize gradient\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # 배치를 GPU에 넣음\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # 배치에서 데이터 추출\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Forwardpropogation               \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask, \n",
    "                        labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "        # Backwardpropogation\n",
    "        loss.backward()\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # weight parameter update from gradient\n",
    "        optimizer.step()\n",
    "        # learning rate 조정\n",
    "        scheduler.step()\n",
    "        # initialize gradient\n",
    "        model.zero_grad()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # no calculate gradient in validation\n",
    "        with torch.no_grad():     \n",
    "            # Forward\n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)\n",
    "        logits = outputs[0]\n",
    "        # CPU로 데이터 이동\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # compare output with ouput label and calculate accuracy\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd952e31-c560-4d1b-b503-aa975b324685",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = random.sample(df[['sentence', 'label']].values.tolist(), 100)\n",
    "test_sent = ['[CLS] ' + t[0] + '[SEP]' for t in test_dataset]\n",
    "test_label = [t[1] for t in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e94e36d2-178b-4e9a-ab4e-7f1b042412bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokened_test_sent = [tokenizer.tokenize(sent) for sent in test_sent]\n",
    "test_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokened_test_sent]\n",
    "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef88bb1d-18e5-4a23-9b98-ecd2e8e60225",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_attention_masks = []\n",
    "for seq in test_input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    test_attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bc69167-cbf3-4ff8-92e4-a73ec25fcc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = torch.tensor(test_input_ids)\n",
    "test_labels = torch.tensor(test_label)\n",
    "test_masks = torch.tensor(test_attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b3a2ca8-dcd4-4318-9487-9727e4c7d70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = WeightedRandomSampler(test_data.weights, test_data.data_size)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a25bb50d-9171-4335-9416-382c211abc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.95\n",
      "Test took: 0:00:01\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "t0 = time.time()\n",
    "model.eval()\n",
    "\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    if step % 100 == 0 and not step == 0:\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():     \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0dd6038-ee33-43a6-aa18-266786b7523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_input_data(sentences):\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "    MAX_LEN = 512\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    attention_masks = []\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "    inputs = torch.tensor(input_ids)\n",
    "    masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return inputs, masks\n",
    "\n",
    "def test_sentences(sentences):\n",
    "    model.eval()\n",
    "    inputs, masks = convert_input_data(sentences)\n",
    "    b_input_ids = inputs.to(device)\n",
    "    b_input_mask = masks.to(device)\n",
    "            \n",
    "    with torch.no_grad():     \n",
    "        outputs = model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04d102fe-dfa1-4110-aee8-4eaf4d0125f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '명소', 1: '카페', 2: '공원', 3: '음식', 4: '술집', 5: '숙박', 6: '문화예술', 7: '시장'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = dict(sorted(mapping.items()))\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7246db97-5357-43f7-8128-f712533f8a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하루건너 하루 오는 지니에요 요즘 아주 열심히 돌아다니고 있다 눙! 제가 스스로 정한 한달에 한번있는 문화WEEK에요. 일주일 동안 온갖 문화생활 몰아서 하는 주 그래서 오늘 금요일을 끝으로 이번 달 자체문화위크도 끝나여. 흑 사실 이번주 아니라고 한 달 내내 아무것도 안하는 건 아니지만 죄책감 없이 놀러다닐 수 있게 스스로 만들어놓은 변명같은거라서끅 그래도 문화생활은 좋은거잖아여! 마음의 양식을 쌓기 위해 정기적으로 꼭 다녀와야해여. 암튼 오늘은 또 초대권을 받게되어서 엄마와 함께 코엑스 아트홀로 연극 라이어 보고왔어요. 요즘 아주 자주 가는거같은 코엑스! 무대사진! 찔끔 더 가까이! 멋있는 조명장 들! 2층이 있긴한데 좌석은 아니였어요! 전부 1층좌석임요 대학로에서 봤었을때보다 좌석이 푹신푹신해서 더 좋았어용~ 티켓인증! 초점 나갔다! 다시찍기! 올해는 뮤지컬은 몇번 봤어도 연극은 거의 안본 거 같아여. 흑흑 왜.? 암튼 간만에 바로 눈 앞에서 배우들 연기하시는거 보니 좋았어요! 스탠리역 배우분이 찌질찌질하시면서도 귀여우시 눙. 시리즈 2탄 3탄은 못봤으면서 1편만 두 번이나 보게되었네요 조만간 나머지 시리즈도 봐야게써요 신난답! 저는 이만 자러가야겠어용 굿밤~ 코엑스 아트홀 서울 강남구 삼성1동\n",
      "문화예술\n",
      "[[-1.5527267 -1.440852  -2.063234  -1.5980655 -1.6387876 -1.7272673\n",
      "  11.917907  -1.8195696]]\n",
      "문화예술\n"
     ]
    }
   ],
   "source": [
    "test_s = random.sample(df[['sentence', 'label']].values.tolist(), 1)[0]\n",
    "print(test_s[0])\n",
    "print(mapping[test_s[1]])\n",
    "logits = test_sentences([test_s[0]])\n",
    "\n",
    "print(logits)\n",
    "print(mapping[np.argmax(logits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53c18966-6957-465c-9b51-fe279a8e954d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./finetune_multiclss_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94f6f0ea-cbc9-4b78-a528-c567fefc9059",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = BertForSequenceClassification.from_pretrained(\n",
    "    'finetue_multiclss_model',\n",
    "    num_labels=8\n",
    ")\n",
    "device = \"cuda:0\"\n",
    "test_model = test_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3622bcd5-36e0-41cf-b59f-e779e6393710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentences_t(sentences):\n",
    "    test_model.eval()\n",
    "    inputs, masks = convert_input_data(sentences)\n",
    "    b_input_ids = inputs.to(device)\n",
    "    b_input_mask = masks.to(device)\n",
    "            \n",
    "    with torch.no_grad():     \n",
    "        outputs = test_model(b_input_ids, \n",
    "                        token_type_ids=None, \n",
    "                        attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0673cb9c-7438-4a79-8147-1b3fbb322e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연꽃 아침산책 하늘사진오늘은 비가 소강상태라 우산작은거 하나 들고 나갔다왔는데요. 저녁식사를 안하고 잤더니 걷는데 좀 허기져서 힘들었어요. 아침부터 고기 구워 먹을까봐요. 꼬리조팝 모감주꽃이 지고 씨앗을 준비했네요. 모감주씨앗이 들어있어요. 비비추꽃이 많아요 생태체험관에서 키우는 가지 방토가 주렁주렁 저 뒤로 롯데타워가 보이네요 병꽃 배롱나무 분홍꽃 애기사과나무를 타고 올라간 찔레장미 핑크뮬리 가을에 색 바뀌면 예쁘겠죠 장미화원 배롱나무 메리골드 노란색이 약간 비맞아서 흐려졌어요 천리향 천리향. 노란 작은게 꽃이겠죠. 일일초를 주로 바늘꽃 목수국\n",
      "공원\n",
      "[[ 0.8995074 -0.8334404 10.280355  -1.471694  -2.417973  -1.4988086\n",
      "  -1.8547026 -1.6283876]]\n",
      "공원\n"
     ]
    }
   ],
   "source": [
    "test_s = random.sample(df[['sentence', 'label']].values.tolist(), 1)[0]\n",
    "print(test_s[0])\n",
    "print(mapping[test_s[1]])\n",
    "logits = test_sentences_t([test_s[0]])\n",
    "\n",
    "print(logits)\n",
    "print(mapping[np.argmax(logits)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd2416ec-48ca-4656-a8a1-ac337430bc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>category</th>\n",
       "      <th>pred</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>백년고기 내가 알바하던 편의점 점주님의 아들이 나와 동갑에, 같은 대학을 다닌다는 ...</td>\n",
       "      <td>3</td>\n",
       "      <td>음식</td>\n",
       "      <td>음식</td>\n",
       "      <td>[-2.2105129, -1.7844205, -3.0720518, 10.252276...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>안녕 얘들아~! 다들 방학해서 좋으니? 난 잘 모르겠어.~ 어차피 학교 가면 잘 수...</td>\n",
       "      <td>3</td>\n",
       "      <td>음식</td>\n",
       "      <td>카페</td>\n",
       "      <td>[0.7887645, 3.412375, -3.0511878, 2.6259508, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>젤라또 아이스크림은 주변에서 쉽게먹을 수 있는 아이스크림은 아니기에김난노와 나는 여...</td>\n",
       "      <td>1</td>\n",
       "      <td>카페</td>\n",
       "      <td>카페</td>\n",
       "      <td>[-1.8647114, 11.358135, -2.1183596, -0.8450916...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>오전부터 비바람이몰아치더니 오후되니이렇게 날씨가 개었네요~ 꽃샘추위 넘나 춥다요 볼...</td>\n",
       "      <td>1</td>\n",
       "      <td>카페</td>\n",
       "      <td>카페</td>\n",
       "      <td>[-1.4839631, 11.0343275, -2.9041655, -0.765607...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>주말 잘 보내고 계신가요? 전 일요일답게 늦게일어나서 이제 아침을 먹었다는. 오늘의...</td>\n",
       "      <td>3</td>\n",
       "      <td>음식</td>\n",
       "      <td>음식</td>\n",
       "      <td>[-2.024914, -1.925607, -3.0845895, 10.284386, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label category pred  \\\n",
       "0  백년고기 내가 알바하던 편의점 점주님의 아들이 나와 동갑에, 같은 대학을 다닌다는 ...      3       음식   음식   \n",
       "1  안녕 얘들아~! 다들 방학해서 좋으니? 난 잘 모르겠어.~ 어차피 학교 가면 잘 수...      3       음식   카페   \n",
       "2  젤라또 아이스크림은 주변에서 쉽게먹을 수 있는 아이스크림은 아니기에김난노와 나는 여...      1       카페   카페   \n",
       "3  오전부터 비바람이몰아치더니 오후되니이렇게 날씨가 개었네요~ 꽃샘추위 넘나 춥다요 볼...      1       카페   카페   \n",
       "4  주말 잘 보내고 계신가요? 전 일요일답게 늦게일어나서 이제 아침을 먹었다는. 오늘의...      3       음식   음식   \n",
       "\n",
       "                                               score  \n",
       "0  [-2.2105129, -1.7844205, -3.0720518, 10.252276...  \n",
       "1  [0.7887645, 3.412375, -3.0511878, 2.6259508, 0...  \n",
       "2  [-1.8647114, 11.358135, -2.1183596, -0.8450916...  \n",
       "3  [-1.4839631, 11.0343275, -2.9041655, -0.765607...  \n",
       "4  [-2.024914, -1.925607, -3.0845895, 10.284386, ...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label_list = []\n",
    "predicted_logit_list = []\n",
    "\n",
    "for text in df['sentence']:\n",
    "    logits = test_sentences_t([text])\n",
    "    predicted_logit_list.append(logits[0])\n",
    "    predicted_label_list.append(mapping[np.argmax(logits)])\n",
    "df['pred'] = predicted_label_list\n",
    "df['score'] = predicted_logit_list\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c91e549-afd9-41a9-aed6-effb13f9d988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['음식', '카페', '명소', '시장', '숙박', '공원', '술집', '문화예술'], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pred'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f8e38d87-278e-46e9-bf1e-247cdc0127b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          공원       0.67      0.97      0.79       560\n",
      "          명소       0.90      0.93      0.91     15606\n",
      "        문화예술       0.73      0.99      0.84      3297\n",
      "          숙박       0.84      0.97      0.90      7354\n",
      "          술집       0.70      0.96      0.81      8286\n",
      "          시장       0.57      0.99      0.73       993\n",
      "          음식       0.99      0.91      0.95     91685\n",
      "          카페       0.93      0.94      0.93     27855\n",
      "\n",
      "    accuracy                           0.93    155636\n",
      "   macro avg       0.79      0.96      0.86    155636\n",
      "weighted avg       0.94      0.93      0.93    155636\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=df['category'], y_pred=df['pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f274ef-6b22-423f-bd5f-81f143b6c44c",
   "metadata": {},
   "source": [
    "https://tutorials.pytorch.kr/beginner/saving_loading_models.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
